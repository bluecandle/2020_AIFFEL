# AIFFEL_9일차 2020.08.04

Tags: AIFFEL_DAILY

### 일정

1. LMS E-3
2. cs231n Lecture 4

# [E-3]뉴스기사 크롤링 및 분류

### **학습 목표**

---

1. HTML 문서의 개념에 대해서 이해한다.
2. 태그의 형식에 대해서 이해한다.
3. 크롤링을 위한 패키지인 BeautifulSoup4의 사용법을 이해한다.
4. 머신 러닝 분류 방법인 나이브 베이즈 분류기의 사용법을 익힌다

**`id`**와 **`class`**라는 **선택자(selector)** 를 추가하면 훨씬 보기도 쉽고, 관리하기 쉬워집니다.

⇒ 그냥 CSS 를 이용한 스타일링을 위한 것으로만 생각을 했었는데, 태그들간의 관계를 나타내기 위한 것으로 생각할 수 있다!

## BeautifulSoup4 && newspaper3k

---

### BeautifulSoup4 위치표시

위치를 표시하는 것은 두 가지 방법이 있습니다.

'>' 를 사용해서 바로 아래에 속해있다를 나타내는 것 ※ 자식이라고도 해요 ( ex. 'body > h1' )

' '(공백)을 사용해서 아래에 속해있다 를 나타내는 것 ※ 자손 이라고도 합니다 ( ex. 'body h1' )

(아래에는 있지만... 바로 아래인지 몇 번 아래인지는 잘 모르겠다는 의미에요)

[출처]([https://m.blog.naver.com/kiddwannabe/221177292446](https://m.blog.naver.com/kiddwannabe/221177292446))

### 결과 출력을 위한 key,value setting

그런데 그 전에 현재는 함수를 호출할 때 각 카테고리의 번호로 호출하고 있는데, 앞으로 결과를 확인할 때 코드로부터 바로 어떤 카테고리인지 확인하기 쉽도록 code를 키, 실제 카테고리를 밸류로 가지는 딕셔너리를 만들어두겠습니다. 이번 실습에 사용할 카테고리들에 대해서만 생성했습니다.

idx2word = {'101' : '경제', '102' : '사회', '103' : '생활/문화', '105' : 'IT/과학'}

### 데이터 샘플 돌려보기

10개의 샘플이 출력해 보았는데, 3개의 카테고리가 전부 존재하는 것을 확인할 수 있습니다. df.sample(10)을 여러 번 호출하면서 달라지는 결과를 확인해 보세요. 수집한 데이터의 샘플들을 랜덤으로 여러 번 출력해 보는 것은 데이터를 파악하는데 꽤 큰 도움이 됩니다.

### 데이터 정제

모델의 훈련에 사용하기 위해서는 우선 데이터를 정제하고, 그 다음 컴퓨터가 좋아하는 숫자의 형태로 바꾸어 주는 작업이 필요합니다.

현재 저장된 뉴스 기사는 기본적으로 각종 숫자, 영어, \n와 같은 HTML 태그들이 섞여 있습니다. 우선, 한글 외에는 전부 제거하도록 정규 표현식을 용하여 전처리를 진행합니다.

```python
# 정규 표현식을 이용해서 한글 외의 문자는 전부 제거합니다.
df['news'] = df['news'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")
df['news']
```

```python
# 그리고, null 이 없는지 확인하기 (저번 시간에도 했던거)
print(df.isnull().sum())
# 중복 제거하기
df.drop_duplicates(subset=['news'], inplace=True)
```

### **토큰화**

---

 자연어 처리에서 대부분의 경우 문자열은 특정 단위로 나누어져야만 합니다. 자연어 처리에서는 이 특정 단위를 '토큰(token)'이라고 하며 이 과정을 토큰화(tokenization) 또는 토크나이징(tokenizing)이라고 합니다.

한국어 자연어처리의 경우에는 토큰화 과정을 주로 형태소 분석기를 사용해서 수행합니다.

[영어와 한국어의 토큰화 차이]([https://wikidocs.net/21698](https://wikidocs.net/21698))

---

보통 토큰화 작업은 단순히 구두점이나 특수문자를 전부 제거하는 정제(cleaning) 작업을 수행하는 것만으로 해결되지 않습니다. 구두점이나 특수문자를 전부 제거하면 토큰이 의미를 잃어버리는 경우가 발생하기도 합니다. 심지어 띄어쓰기 단위로 자르면 사실상 단어 토큰이 구분되는 영어와 달리, 한국어는 띄어쓰기만으로는 단어 토큰을 구분하기 어렵습니다

원하는 결과가 나오도록 토큰화 도구를 직접 설계할 수도 있겠지만, 기존에 공개된 도구들을 사용하였을 때의 결과가 사용자의 목적과 일치한다면 해당 도구를 사용할 수도 있을 것입니다. NLTK는 영어 코퍼스를 토큰화하기 위한 도구들을 제공합니다.

물론, 한국어에 대한 문장 토큰화 도구가 존재합니다. 한국어에 대한 문장 토큰화 도구가 여럿있지만, 여기에서는 박상길님이 개발한 KSS(Korean Sentence Splitter)를 소개합니다.

영어는 New York과 같은 합성어나 he's 와 같이 줄임말에 대한 예외처리만 한다면, 띄어쓰기(whitespace)를 기준으로 하는 띄어쓰기 토큰화를 수행해도 단어 토큰화가 잘 작동합니다. 거의 대부분의 경우에서 단어 단위로 띄어쓰기가 이루어지기 때문에 띄어쓰기 토큰화와 단어 토큰화가 거의 같기 때문입니다.

하지만 한국어는 영어와는 달리 띄어쓰기만으로는 토큰화를 하기에 부족합니다. 한국어의 경우에는 띄어쓰기 단위가 되는 단위를 '어절'이라고 하는데 즉, 어절 토큰화는 한국어 NLP에서 지양되고 있습니다. 어절 토큰화와 단어 토큰화가 같지 않기 때문입니다. 그 근본적인 이유는 한국어가 영어와는 다른 형태를 가지는 언어인 교착어라는 점에서 기인합니다. 교착어란 조사, 어미 등을 붙여서 말을 만드는 언어를 말합니다.

한국어 토큰화에서는 **형태소(morpheme)**란 개념을 반드시 이해해야 합니다.

형태소(morpheme)란 뜻을 가진 가장 작은 말의 단위를 말합니다. 이 형태소에는 두 가지 형태소가 있는데 자립 형태소와 의존 형태소입니다.

- **자립 형태소** : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소. 그 자체로 단어가 된다. 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있다.
- **의존 형태소** : 다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간를 말한다.

---

⇒한국어는 교착어이며, 영어보다 띄어쓰기가 잘 지켜지지 않기 때문(띄어쓰기를 하지 않더라도 이해하는데 어려움이 비교적 크지 않다.)

### **불용어(stopwords) 제거**

---

토큰화 과정에서 해야 하는 전처리가 있습니다. 바로 불필요한 토큰들을 제거하는 불용어(stopwords) 제거입니다.

**불용어** 란, 데이터 전체에서 꽤 많이 등장하지만 실제로는 자연어 처리에 큰 영향을 주지 않는, 중요하지 않은 단어들을 말합니다.

한국어에서는 주로 ~가, ~은, ~는과 같은 조사나 접사 등이 불용어에 속합니다. 또는 해당 데이터의 특성으로 인해 자주 등장할 수밖에 없는 단어들도 불용어입니다.

불용어는 미리 짐작하여 한 번에 정의하는 것이 아니라, 토큰화 과정을 거친 결과를 지속적으로 확인하면서 계속해서 추가하게 되는 것이 일반적입니다.

## 나이브 베이즈 분류기 _ Naive Bayes classifier

---

머신 러닝 모델인 나이브 베이즈 분류기를 사용하기 위해서는 각 뉴스의 텍스트 데이터를 벡터로 변환할 필요가 있습니다. 이를 위한 전처리로 TF-IDF 라는 방법을 사용하겠습니다.

### TF-IDF

Term Frequency

Inverse Document Frequency

목적 : 단어별로 문서와의 연관성을 알아보기 위함! ( 각 단어별로 문서에 대한 정보를 얼마나 가지고 있는지를 수치로 나타내는 것이라고 이해해도 좋다 )

TF : 단어가 문서에 얼마나 자주 출현했는가? 자주 출현하면 문서와 관련이 있을 것이라는 가설 하에!

근데, TF 스코어가 꼭 답이기만 한걸까? 

A friend in need is a friend indeed

⇒ 'a' 처럼 문장 자체와 상관 없이 자주 출현하는 단어들도 있음!

⇒ 그래서 IDF 라는 개념이 등장함

## 실습

---

1. 여러 한글 형태소 분류기에 대해 알아본다.
2. 불용어를 직접 추가해보고, 추가된 불용어와 다른 한글 형태소 분류기를 통해 전처리를 해본다.
3. 1,2 단계의 결과를 비교해본다.