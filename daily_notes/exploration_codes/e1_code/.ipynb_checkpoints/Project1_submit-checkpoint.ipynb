{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages (7.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "# PIL 라이브러리가 설치되어 있지 않다면 설치\n",
    "!pip install pillow   \n",
    "\n",
    "# 필요한 라이브러리 가져오기\n",
    "from PIL import Image\n",
    "import os, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 시드 고정?? 이렇게 해도 안되는거같음!\n",
    "import random\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "train_seed = 777\n",
    "random.seed(777)\n",
    "tf.random.set_seed(train_seed)\n",
    "np.random.seed(train_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 크기 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 디렉토리 경로:  /home/aiffel0042/aiffel/rock_scissor_paper/rcp_all/scissor_all\n",
      "scissor_all 이미지 resize 완료!\n",
      "이미지 디렉토리 경로:  /home/aiffel0042/aiffel/rock_scissor_paper/rcp_all/rock_all\n",
      "rock_all 이미지 resize 완료!\n",
      "이미지 디렉토리 경로:  /home/aiffel0042/aiffel/rock_scissor_paper/rcp_all/paper_all\n",
      "paper_all 이미지 resize 완료!\n",
      "이미지 디렉토리 경로:  /home/aiffel0042/aiffel/rock_scissor_paper/rcp_all/scissor_test\n",
      "scissor_test 이미지 resize 완료!\n",
      "이미지 디렉토리 경로:  /home/aiffel0042/aiffel/rock_scissor_paper/rcp_all/rock_test\n",
      "rock_test 이미지 resize 완료!\n",
      "이미지 디렉토리 경로:  /home/aiffel0042/aiffel/rock_scissor_paper/rcp_all/paper_test\n",
      "paper_test 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convertImageSize(name):\n",
    "    image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rcp_all/\"+name\n",
    "    print(\"이미지 디렉토리 경로: \", image_dir_path)\n",
    "\n",
    "    images=glob.glob(image_dir_path + \"/*.jpg\")  \n",
    "\n",
    "    img_size = 28\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "    target_size=(img_size,img_size)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img,\"JPEG\")\n",
    "\n",
    "    print(\"{} 이미지 resize 완료!\".format(name))\n",
    "\n",
    "dirs = ['scissor_all','rock_all','paper_all','scissor_test','rock_test','paper_test']\n",
    "for c in dirs:\n",
    "    convertImageSize(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전부 불러오고, ~~train/test 데이터 분리~~\n",
    "[데이터 분리 참조](https://rfriend.tistory.com/519)\n",
    "\n",
    "원래 5조의 전체 데이터 (2100장씩 6300장) 를 임의로 train, test 데이터로 구분하여 사용하였음.    \n",
    "근데, 그렇게 하면 정확도가 약 99% 정도가 나오는 비정상적인 상황이 발생.    \n",
    "따라서, 다른 인원의 데이터를 test dataset 으로 사용할 필요를 느껴서, 변경하였음. (이영빈님 데이터 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all 이미지 개수는 6300 입니다.\n",
      "x_train shape: (6300, 28, 28, 3)\n",
      "y_train shape: (6300,)\n",
      "test 이미지 개수는 300 입니다.\n",
      "x_test shape: (300, 28, 28, 3)\n",
      "y_test shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "def load_data(isTest,img_path):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    number_of_data=2100*3   # 데이터는 2100개씩 있음\n",
    "    if(isTest):\n",
    "        number_of_data= 100 * 3\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    suffix = 'all'\n",
    "    if(isTest):\n",
    "        suffix = 'test'\n",
    "        \n",
    "    for file in glob.iglob(img_path+'/scissor_{}/*.jpg'.format(suffix)):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock_{}/*.jpg'.format(suffix)):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper_{}/*.jpg'.format(suffix)):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"{} 이미지 개수는\".format(suffix),idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rcp_all/\"\n",
    "(x_train, y_train)=load_data(False,image_dir_path)\n",
    "\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "\n",
    "(x_test, y_test)=load_data(True,image_dir_path)\n",
    "\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))\n",
    "\n",
    "\n",
    "## train,test 구분하여 사용할 때 사용하였음\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_whole,y_whole, test_size=0.1, shuffle=True, random_state=500)\n",
    "# print(\"x_train shape: {}\".format(x_train.shape))\n",
    "# print(\"x_test shape: {}\".format(x_test.shape))\n",
    "# print(\"y_train shape: {}\".format(y_train.shape))\n",
    "# print(\"y_test shape: {}\".format(y_test.shape))\n",
    "# x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "# x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.imshow(x_train[601])\n",
    "# print('라벨: ', y_train[601])\n",
    "plt.imshow(x_test[201])\n",
    "print('라벨: ', y_test[201])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 네트워크 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수:  7\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 122,051\n",
      "Trainable params: 122,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_channel_1=32\n",
    "n_channel_2=64\n",
    "n_dense=64\n",
    "n_train_epoch=20\n",
    "\n",
    "n_image_channel = 3\n",
    "n_classes = 3\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,n_image_channel)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(n_classes, activation='softmax'))\n",
    "print('Model에 추가된 Layer 개수: ', len(model.layers))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 네트워크 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 0.9973\n",
      "Epoch 2/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 1.3975e-04 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 7.2077e-05 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 5.5467e-05 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 4.4331e-05 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 3.7927e-05 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 3.2901e-05 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 2.8560e-05 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 2.5446e-05 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 2.3659e-05 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 2.1747e-05 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 1.9380e-05 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 1.7750e-05 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 1.6171e-05 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 1.4797e-05 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 1.3457e-05 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 1.2833e-05 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 1.1239e-05 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 1.0325e-05 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 9.5525e-06 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd295c656d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=n_train_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트\n",
    "테스트 데이터는 이미 만들어져 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape: (300, 28, 28, 3)\n",
      "y_test shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss, accuracy 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 1.3841 - accuracy: 0.7433\n",
      "===================== result ================== \n",
      "n_channel_1: 32\n",
      "n_channel_2: 64\n",
      "n_dense: 64\n",
      "n_train_epoch: 20\n",
      "test_loss: 1.3841452598571777 \n",
      "test_accuracy: 0.7433333396911621\n",
      "model.predict() 결과 :  [0.00112568 0.25241578 0.7464585 ]\n",
      "model이 추론한 가장 가능성이 높은 결과 :  2\n",
      "실제 데이터의 라벨 :  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test_norm,y_test, verbose=2)\n",
    "print(\"===================== result ================== \")\n",
    "print(\"n_channel_1: {}\".format(n_channel_1))\n",
    "print(\"n_channel_2: {}\".format(n_channel_2))\n",
    "print(\"n_dense: {}\".format(n_dense))\n",
    "print(\"n_train_epoch: {}\".format(n_train_epoch))\n",
    "\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))\n",
    "\n",
    "predicted_result = model.predict(x_test_norm)  # model이 추론한 확률값. \n",
    "predicted_labels = np.argmax(predicted_result, axis=1)\n",
    "\n",
    "idx=3  #4번째 x_test를 살펴보자. \n",
    "print('model.predict() 결과 : ', predicted_result[idx])\n",
    "print('model이 추론한 가장 가능성이 높은 결과 : ', predicted_labels[idx])\n",
    "print('실제 데이터의 라벨 : ', y_test[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 더 좋은 네트워크 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수:  8\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                51232     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 70,723\n",
      "Trainable params: 70,723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.8329 - accuracy: 0.6094\n",
      "Epoch 2/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.4126 - accuracy: 0.8378\n",
      "Epoch 3/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.2765 - accuracy: 0.8989\n",
      "Epoch 4/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.1921 - accuracy: 0.9314\n",
      "Epoch 5/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.1485 - accuracy: 0.9513\n",
      "Epoch 6/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.1028 - accuracy: 0.9660\n",
      "Epoch 7/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.0933 - accuracy: 0.9695\n",
      "Epoch 8/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.0690 - accuracy: 0.9776\n",
      "Epoch 9/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.0555 - accuracy: 0.9817\n",
      "Epoch 10/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.0470 - accuracy: 0.9859\n",
      "Epoch 11/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.0421 - accuracy: 0.9868\n",
      "Epoch 12/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.0371 - accuracy: 0.9865\n",
      "Epoch 13/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.0340 - accuracy: 0.9889\n",
      "Epoch 14/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.0294 - accuracy: 0.9906\n",
      "Epoch 15/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.0338 - accuracy: 0.9897\n",
      "Epoch 16/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.0254 - accuracy: 0.9927\n",
      "Epoch 17/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.0280 - accuracy: 0.9905\n",
      "Epoch 18/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.0254 - accuracy: 0.9919\n",
      "Epoch 19/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.9943\n",
      "Epoch 20/20\n",
      "197/197 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 0.9965\n",
      "10/10 - 0s - loss: 1.1192 - accuracy: 0.7333\n",
      "===================== result ================== \n",
      "n_channel_1: 32\n",
      "n_channel_2: 64\n",
      "n_dense: 32\n",
      "n_train_epoch: 20\n",
      "test_loss: 1.1192432641983032 \n",
      "test_accuracy: 0.7333333492279053\n"
     ]
    }
   ],
   "source": [
    "n_channel_1= 32\n",
    "n_channel_2= 64\n",
    "n_dense= 32\n",
    "n_train_epoch= 20\n",
    "\n",
    "n_image_channel = 3\n",
    "n_classes = 3\n",
    "# 모델 설계\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,n_image_channel)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(n_classes, activation='softmax'))\n",
    "print('Model에 추가된 Layer 개수: ', len(model.layers))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 모델 훈련\n",
    "model.compile(optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=n_train_epoch)\n",
    "\n",
    "# 모델 테스트\n",
    "test_loss, test_accuracy = model.evaluate(x_test_norm,y_test, verbose=2)\n",
    "print(\"===================== result ================== \")\n",
    "print(\"n_channel_1: {}\".format(n_channel_1))\n",
    "print(\"n_channel_2: {}\".format(n_channel_2))\n",
    "print(\"n_dense: {}\".format(n_dense))\n",
    "print(\"n_train_epoch: {}\".format(n_train_epoch))\n",
    "\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))\n",
    "\n",
    "\n",
    "# predicted_result = model.predict(x_test_norm)  # model이 추론한 확률값. \n",
    "# predicted_labels = np.argmax(predicted_result, axis=1)\n",
    "\n",
    "# idx=3  #4번째 x_test를 살펴보자. \n",
    "# print('model.predict() 결과 : ', predicted_result[idx])\n",
    "# print('model이 추론한 가장 가능성이 높은 결과 : ', predicted_labels[idx])\n",
    "# print('실제 데이터의 라벨 : ', y_test[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가장 높은 정확도 결과\n",
    "\n",
    "n_channel_1: 32\n",
    "n_channel_2: 64\n",
    "n_dense: 64\n",
    "n_train_epoch: 20\n",
    "test_loss: 1.3841452598571777 \n",
    "test_accuracy: 0.7433333396911621"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과 보고서\n",
    "========\n",
    "\n",
    "## 데이터 수집\n",
    "원래 직접 찍은 사진 900장 (각 300장)을 훈련 데이터셋으로 사용하고, 다른 조원의 데이터 2100장(각 700장)을 테스트 데이터셋으로 사용하였다.\n",
    "그런데, 파이퍼파라미터를 여러 방향으로 수정해보아도 결과 값이 대략 45~55% 정도로 나타났어서, 성능을 향상시킬 방법을 해각하게 되었다.\n",
    "훈련 데이터를 더 많이 투입시키는 방법이 좋겠다고 생각하여, 조원들의 데이터를 다 모았다. (2100장씩 6300장)\n",
    "그리고, 해당 데이터를 임의로 8:2, 9:1 비율로 나누어 훈련 데이터셋과 테스트 데이터셋으로 사용하였다.\n",
    "\n",
    "## 비슷한 특징을 가진 이미지의 문제점\n",
    "조원들의 전체 데이터를 하나의 풀로 두고 그 안에서 훈련, 테스트 데이터를 나누고 학습을 진행하니, 약 99%에 가까운 결과들이 지속적으로 나타났다.\n",
    "그래서, 비슷한 환경에서 찍은 사진들, 즉 비슷한 특징을 가진 이미지들을 임의로 나누어 훈련 및 테스트를 진행하는 것이 유의미한 작업은 아니라고 생각되었다.\n",
    "\n",
    "## 다른 사람의 데이터를 테스트 데이터로 사용하자.\n",
    "그래서, 슬랙에 올라와 있는 다른 분의 데이터셋을 테스트 데이터로 사용하고, 조 전체 데이터는 훈련 데이터로 사용하였다.\n",
    "그랬더니, 역시나 이전의 결과보다는 조금 더 낮은 정확도가 출력되었다.\n",
    "\n",
    "### 의문점\n",
    "하이퍼파라미터가 모두 동일하여도, 모델을 학습시킬 때마다 훈련의 결과가 다르게 나타나는 것으로 보였다.\n",
    "슬랙 내에서 해당 내용에 대한 논의가 이루어지고 있기도 한 것으로 보였는데, 추후 설명이 있었으면 좋겠다고 생각한다.\n",
    "seed 고정과의 관련성??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
