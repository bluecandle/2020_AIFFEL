{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트: 단어 level로 번역기 업그레이드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:43.371307Z",
     "start_time": "2020-09-22T09:43:41.249890Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Embedding, Masking,LSTM,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:43.374089Z",
     "start_time": "2020-09-22T09:43:43.372537Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:43.696509Z",
     "start_time": "2020-09-22T09:43:43.375331Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "전체 샘플의 수 : 178009\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                              eng  \\\n150453  They supported his right to speak freely.   \n97028              He wanted it to be a surprise.   \n97836              I fully support your proposal.   \n114305           You are a good cook, aren't you?   \n68671                  I can deliver that to Tom.   \n\n                                                   fra  \\\n150453  Elles soutinrent son droit à parler librement.   \n97028               Il voulut que ce fut une surprise.   \n97836        Je soutiens intégralement ta proposition.   \n114305                      Tu es bon cuisinier, non ?   \n68671                       Je peux livrer cela à Tom.   \n\n                                                       cc  \n150453  CC-BY 2.0 (France) Attribution: tatoeba.org #8...  \n97028   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n97836   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n114305  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n68671   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eng</th>\n      <th>fra</th>\n      <th>cc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>150453</th>\n      <td>They supported his right to speak freely.</td>\n      <td>Elles soutinrent son droit à parler librement.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #8...</td>\n    </tr>\n    <tr>\n      <th>97028</th>\n      <td>He wanted it to be a surprise.</td>\n      <td>Il voulut que ce fut une surprise.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n    </tr>\n    <tr>\n      <th>97836</th>\n      <td>I fully support your proposal.</td>\n      <td>Je soutiens intégralement ta proposition.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n    <tr>\n      <th>114305</th>\n      <td>You are a good cook, aren't you?</td>\n      <td>Tu es bon cuisinier, non ?</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n    </tr>\n    <tr>\n      <th>68671</th>\n      <td>I can deliver that to Tom.</td>\n      <td>Je peux livrer cela à Tom.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "file_path = './data/fra.txt'\n",
    "original_lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(original_lines))\n",
    "original_lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정제, 정규화, 전처리 (영어,프랑스어 모두!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 33000개만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:43.708075Z",
     "start_time": "2020-09-22T09:43:43.697413Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                        eng                              fra\n29698  I hate taking risks.  Je déteste prendre des risques.\n17361     What shall we do?                      Que faire ?\n14060     Everyone laughed.              Tout le monde a ri.\n20562    My brother is out.            Mon frère est dehors.\n16046     No one ever came.         Personne ne vint jamais.",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eng</th>\n      <th>fra</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>29698</th>\n      <td>I hate taking risks.</td>\n      <td>Je déteste prendre des risques.</td>\n    </tr>\n    <tr>\n      <th>17361</th>\n      <td>What shall we do?</td>\n      <td>Que faire ?</td>\n    </tr>\n    <tr>\n      <th>14060</th>\n      <td>Everyone laughed.</td>\n      <td>Tout le monde a ri.</td>\n    </tr>\n    <tr>\n      <th>20562</th>\n      <td>My brother is out.</td>\n      <td>Mon frère est dehors.</td>\n    </tr>\n    <tr>\n      <th>16046</th>\n      <td>No one ever came.</td>\n      <td>Personne ne vint jamais.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "num_samples = 33000\n",
    "lines = original_lines[['eng', 'fra']][:num_samples] # 3.3만개 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 소문자로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:43.733534Z",
     "start_time": "2020-09-22T09:43:43.708867Z"
    }
   },
   "outputs": [],
   "source": [
    "# lines['eng_separate']\n",
    "lines['eng'] = lines['eng'].str.lower()\n",
    "lines['fra'] = lines['fra'].str.lower()\n",
    "# lines['eng_separate'].str.toLowerCase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:43.738623Z",
     "start_time": "2020-09-22T09:43:43.734479Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0                         go.\n1                         hi.\n2                         hi.\n3                        run!\n4                        run!\n                 ...         \n32995    what was their goal?\n32996    what were you doing?\n32997    what would tom need?\n32998    what would you like?\n32999    what would you like?\nName: eng, Length: 33000, dtype: object"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "lines['eng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:43.749994Z",
     "start_time": "2020-09-22T09:43:43.739481Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0                                   va !\n1                                salut !\n2                                 salut.\n3                                cours !\n4                               courez !\n                      ...               \n32995              quel était leur but ?\n32996    qu'étais-tu en train de faire ?\n32997     de quoi tom aurait-il besoin ?\n32998                   qu'aimerais-tu ?\n32999                 qu'aimeriez-vous ?\nName: fra, Length: 33000, dtype: object"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "lines['fra']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 띄어쓰기 단위로 구분 && 구두점을 단어와 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:43.860790Z",
     "start_time": "2020-09-22T09:43:43.751471Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# import string\n",
    "# string.punctuation\n",
    "\n",
    "lines['eng_separate'] = lines['eng'].str.findall(r\"[\\w']+|[.,!?;]\")\n",
    "lines['fra_separate'] = lines['fra'].str.findall(r\"[\\w']+|[.,!?;]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:43.866140Z",
     "start_time": "2020-09-22T09:43:43.861803Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0                            [go, .]\n1                            [hi, .]\n2                            [hi, .]\n3                           [run, !]\n4                           [run, !]\n                    ...             \n32995    [what, was, their, goal, ?]\n32996    [what, were, you, doing, ?]\n32997    [what, would, tom, need, ?]\n32998    [what, would, you, like, ?]\n32999    [what, would, you, like, ?]\nName: eng_separate, Length: 33000, dtype: object"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "lines['eng_separate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:43.877557Z",
     "start_time": "2020-09-22T09:43:43.867000Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0                                        [va, !]\n1                                     [salut, !]\n2                                     [salut, .]\n3                                     [cours, !]\n4                                    [courez, !]\n                          ...                   \n32995                [quel, était, leur, but, ?]\n32996    [qu'étais, tu, en, train, de, faire, ?]\n32997     [de, quoi, tom, aurait, il, besoin, ?]\n32998                       [qu'aimerais, tu, ?]\n32999                     [qu'aimeriez, vous, ?]\nName: fra_separate, Length: 33000, dtype: object"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "lines['fra_separate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시작, 종료 토큰 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:43.951730Z",
     "start_time": "2020-09-22T09:43:43.878463Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "# lines.fra = lines.fra.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "# print('전체 샘플의 수 :',len(lines))\n",
    "# lines.sample(5)\n",
    "\n",
    "lines['fra_separate_token'] = lines.fra_separate.apply(\n",
    "    lambda x : ['\\t'] + x + ['\\n']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:43.973097Z",
     "start_time": "2020-09-22T09:43:43.952671Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                      eng                           fra  \\\n14596   i bowed politely.       je m'inclinai poliment.   \n1251          here we go.               on est partis !   \n5296       he's ruthless.            il est sans pitié.   \n20510  long time, no see.           ça fait une paille.   \n18873  how can i do that?  comment puis-je faire cela ?   \n\n                      eng_separate                         fra_separate  \\\n14596      [i, bowed, politely, .]        [je, m'inclinai, poliment, .]   \n1251             [here, we, go, .]                 [on, est, partis, !]   \n5296           [he's, ruthless, .]            [il, est, sans, pitié, .]   \n20510  [long, time, ,, no, see, .]           [ça, fait, une, paille, .]   \n18873   [how, can, i, do, that, ?]  [comment, puis, je, faire, cela, ?]   \n\n                                fra_separate_token  \n14596        [\\t, je, m'inclinai, poliment, ., \\n]  \n1251                  [\\t, on, est, partis, !, \\n]  \n5296             [\\t, il, est, sans, pitié, ., \\n]  \n20510           [\\t, ça, fait, une, paille, ., \\n]  \n18873  [\\t, comment, puis, je, faire, cela, ?, \\n]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eng</th>\n      <th>fra</th>\n      <th>eng_separate</th>\n      <th>fra_separate</th>\n      <th>fra_separate_token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14596</th>\n      <td>i bowed politely.</td>\n      <td>je m'inclinai poliment.</td>\n      <td>[i, bowed, politely, .]</td>\n      <td>[je, m'inclinai, poliment, .]</td>\n      <td>[\\t, je, m'inclinai, poliment, ., \\n]</td>\n    </tr>\n    <tr>\n      <th>1251</th>\n      <td>here we go.</td>\n      <td>on est partis !</td>\n      <td>[here, we, go, .]</td>\n      <td>[on, est, partis, !]</td>\n      <td>[\\t, on, est, partis, !, \\n]</td>\n    </tr>\n    <tr>\n      <th>5296</th>\n      <td>he's ruthless.</td>\n      <td>il est sans pitié.</td>\n      <td>[he's, ruthless, .]</td>\n      <td>[il, est, sans, pitié, .]</td>\n      <td>[\\t, il, est, sans, pitié, ., \\n]</td>\n    </tr>\n    <tr>\n      <th>20510</th>\n      <td>long time, no see.</td>\n      <td>ça fait une paille.</td>\n      <td>[long, time, ,, no, see, .]</td>\n      <td>[ça, fait, une, paille, .]</td>\n      <td>[\\t, ça, fait, une, paille, ., \\n]</td>\n    </tr>\n    <tr>\n      <th>18873</th>\n      <td>how can i do that?</td>\n      <td>comment puis-je faire cela ?</td>\n      <td>[how, can, i, do, that, ?]</td>\n      <td>[comment, puis, je, faire, cela, ?]</td>\n      <td>[\\t, comment, puis, je, faire, cela, ?, \\n]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 케라스 토크나이저로 텍스트를 숫자로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:44.105657Z",
     "start_time": "2020-09-22T09:43:43.974001Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[27, 1], [1160, 1], [1160, 1]]"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# tf.keras.preprocessing.text.Tokenizer(\n",
    "#     num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,\n",
    "#     split=' ', char_level=False, oov_token=None, document_count=0, **kwargs\n",
    "# )\n",
    "\n",
    "eng_tokenizer = Tokenizer(char_level=False)   # 단어 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng_separate)               # 33000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng_separate)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:44.345943Z",
     "start_time": "2020-09-22T09:43:44.106506Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[1, 82, 10, 2], [1, 1084, 10, 2], [1, 1084, 3, 2]]"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=False)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra_separate_token)                 # 50000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra_separate_token)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:44.349326Z",
     "start_time": "2020-09-22T09:43:44.346772Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\t'"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "fra_tokenizer.index_word[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어장 크기 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:44.359914Z",
     "start_time": "2020-09-22T09:43:44.350023Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "영어 단어장의 크기 : 4796\n프랑스어 단어장의 크기 : 9034\n"
    }
   ],
   "source": [
    "# 0번 토큰을 고려한 +1\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:44.373303Z",
     "start_time": "2020-09-22T09:43:44.360656Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "영어 시퀀스의 최대 길이 8\n프랑스어 시퀀스의 최대 길이 15\n"
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:44.383453Z",
     "start_time": "2020-09-22T09:43:44.374062Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "전체 샘플의 수 : 33000\n영어 단어장의 크기 : 4796\n프랑스어 단어장의 크기 : 9034\n영어 시퀀스의 최대 길이 8\n프랑스어 시퀀스의 최대 길이 15\n"
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  프랑스어 시퀀스 2개 버전 준비\n",
    "- 하나는 디코더의 출력과 비교해야 할 **정답 데이터**로 사용해야 할 원래 목적에 따른 것입니다.\n",
    "- 하나는 이전 스텝에서 언급했던 **교사 강요(Teacher forcing)을 위해** **디코더의 입력**으로 사용하기 위한 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:44.450757Z",
     "start_time": "2020-09-22T09:43:44.384123Z"
    }
   },
   "outputs": [],
   "source": [
    "# endocer input은 input_text 그대로\n",
    "encoder_input = input_text\n",
    "\n",
    "# 종료 토큰 제거 ( 교사 강요를 위해 decoder input으로 들어가는거니까 )\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거 ( decoder 출력과 비교하는 값이니까 )\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:44.682382Z",
     "start_time": "2020-09-22T09:43:44.452188Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "영어 데이터의 크기(shape) : (33000, 8)\n프랑스어 입력데이터의 크기(shape) : (33000, 15)\n프랑스어 출력데이터의 크기(shape) : (33000, 15)\n"
    }
   ],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:44.685489Z",
     "start_time": "2020-09-22T09:43:44.683160Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[27  1  0  0  0  0  0  0]\n"
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원 핫 인코딩\n",
    "단어 단위로 하니까 안한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:45.627251Z",
     "start_time": "2020-09-22T09:43:44.686399Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoder_input = to_categorical(encoder_input)\n",
    "# decoder_input = to_categorical(decoder_input)\n",
    "# decoder_target = to_categorical(decoder_target)\n",
    "# print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "# print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "# print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 쪼개기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:45.632574Z",
     "start_time": "2020-09-22T09:43:45.628983Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "영어 학습데이터의 크기(shape) : (30000, 8)\n프랑스어 학습 입력데이터의 크기(shape) : (30000, 15)\n프랑스어 학습 출력데이터의 크기(shape) : (30000, 15)\n"
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input_train))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input_train))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 층(Embedding layer) 사용하기\n",
    "https://wikidocs.net/33793\n",
    "\n",
    "어떤 단어 → 단어에 부여된 고유한 정수값 → 임베딩 층 통과 → 밀집 벡터   \n",
    "\n",
    "임베딩 층은 입력 정수에 대해 밀집 벡터(dense vector)로 맵핑하고 이 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련됩니다. 훈련 과정에서 단어는 모델이 풀고자하는 작업에 맞는 값으로 업데이트 됩니다. 그리고 이 밀집 벡터를 임베딩 벡터라고 부릅니다.\n",
    "\n",
    "\n",
    "인코더와 디코더의 임베딩 층은 서로 다른 임베딩 층을 사용해야 하지만,   \n",
    "디코더의 훈련 과정과 테스트 과정(예측 과정)에서의 임베딩 층은 동일해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:46.744261Z",
     "start_time": "2020-09-22T09:43:45.633566Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 인코더에서 사용할 임베딩 층 사용 예시\n",
    "# 입력 텐서 생성 (입력 문장을 저장하게 될 변수 텐서)\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "\n",
    "enc_emb_dim = 256\n",
    "enc_emb =  Embedding(eng_vocab_size, enc_emb_dim,input_length=max_eng_seq_len)(encoder_inputs)\n",
    "\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb)\n",
    "\n",
    "# hidden size는 LSTM의 수용력을 의미한다.\n",
    "encoder_lstm = LSTM(units = 256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# # 입력 텐서 생성.\n",
    "# encoder_inputs = Input(shape=(None, eng_vocab_size))\n",
    "# # hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "# encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "# # 디코더로 전달할 hidden state, cell state를 리턴. encoder_outputs은 여기서는 불필요.\n",
    "# encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# # hidden state와 cell state를 다음 time step으로 전달하기 위해서 별도 저장.\n",
    "# encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:46.747671Z",
     "start_time": "2020-09-22T09:43:46.745155Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[<tf.Tensor 'lstm/PartitionedCall:2' shape=(None, 256) dtype=float32>,\n <tf.Tensor 'lstm/PartitionedCall:3' shape=(None, 256) dtype=float32>]"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:46.927859Z",
     "start_time": "2020-09-22T09:43:46.748636Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 입력 텐서 생성\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "dec_emb_dim = 256\n",
    "dec_emb =  Embedding(fra_vocab_size, dec_emb_dim)(decoder_inputs)\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking, initial_state = encoder_states)\n",
    "\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "# # 입력 텐서 생성.\n",
    "# decoder_inputs = Input(shape=(None, fra_vocab_size))\n",
    "# # hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "# decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "# # decoder_outputs는 모든 time step의 hidden state\n",
    "# decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:46.942430Z",
     "start_time": "2020-09-22T09:43:46.928677Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, None, 256)    1227776     input_1[0][0]                    \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, None, 256)    2312704     input_2[0][0]                    \n__________________________________________________________________________________________________\nmasking (Masking)               (None, None, 256)    0           embedding[0][0]                  \n__________________________________________________________________________________________________\nmasking_1 (Masking)             (None, None, 256)    0           embedding_1[0][0]                \n__________________________________________________________________________________________________\nlstm (LSTM)                     [(None, 256), (None, 525312      masking[0][0]                    \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   [(None, None, 256),  525312      masking_1[0][0]                  \n                                                                 lstm[0][1]                       \n                                                                 lstm[0][2]                       \n__________________________________________________________________________________________________\ndense (Dense)                   (None, None, 9034)   2321738     lstm_1[0][0]                     \n==================================================================================================\nTotal params: 6,912,842\nTrainable params: 6,912,842\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:46.950269Z",
     "start_time": "2020-09-22T09:43:46.943244Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\",  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T09:43:46.959127Z",
     "start_time": "2020-09-22T09:43:46.951061Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-22T09:43:41.326Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/50\n938/938 [==============================] - 20s 21ms/step - loss: 1.6382 - acc: 0.7522 - val_loss: 1.6675 - val_acc: 0.7548\nEpoch 2/50\n938/938 [==============================] - 18s 20ms/step - loss: 1.2291 - acc: 0.8112 - val_loss: 1.4896 - val_acc: 0.7805\nEpoch 3/50\n938/938 [==============================] - 18s 20ms/step - loss: 1.1031 - acc: 0.8283 - val_loss: 1.4009 - val_acc: 0.7906\nEpoch 4/50\n938/938 [==============================] - 18s 20ms/step - loss: 1.0154 - acc: 0.8403 - val_loss: 1.3486 - val_acc: 0.7980\nEpoch 5/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.9505 - acc: 0.8493 - val_loss: 1.3157 - val_acc: 0.8026\nEpoch 6/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.8977 - acc: 0.8585 - val_loss: 1.2895 - val_acc: 0.8096\nEpoch 7/50\n938/938 [==============================] - 19s 20ms/step - loss: 0.8572 - acc: 0.8659 - val_loss: 1.2812 - val_acc: 0.8104\nEpoch 8/50\n938/938 [==============================] - 19s 20ms/step - loss: 0.8262 - acc: 0.8722 - val_loss: 1.2748 - val_acc: 0.8143\nEpoch 9/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.8036 - acc: 0.8780 - val_loss: 1.2865 - val_acc: 0.8168\nEpoch 10/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.7916 - acc: 0.8830 - val_loss: 1.2917 - val_acc: 0.8192\nEpoch 11/50\n938/938 [==============================] - 19s 20ms/step - loss: 0.7732 - acc: 0.8871 - val_loss: 1.2933 - val_acc: 0.8220\nEpoch 12/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.7568 - acc: 0.8914 - val_loss: 1.3053 - val_acc: 0.8209\nEpoch 13/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.7467 - acc: 0.8948 - val_loss: 1.3054 - val_acc: 0.8219\nEpoch 14/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.7380 - acc: 0.8974 - val_loss: 1.3157 - val_acc: 0.8201\nEpoch 15/50\n938/938 [==============================] - 19s 20ms/step - loss: 0.7269 - acc: 0.8997 - val_loss: 1.3095 - val_acc: 0.8228\nEpoch 16/50\n938/938 [==============================] - 19s 20ms/step - loss: 0.7112 - acc: 0.9021 - val_loss: 1.3153 - val_acc: 0.8224\nEpoch 17/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.7039 - acc: 0.9042 - val_loss: 1.3219 - val_acc: 0.8223\nEpoch 18/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.6993 - acc: 0.9062 - val_loss: 1.3265 - val_acc: 0.8225\nEpoch 19/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.6954 - acc: 0.9075 - val_loss: 1.3339 - val_acc: 0.8230\nEpoch 20/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.6860 - acc: 0.9089 - val_loss: 1.3353 - val_acc: 0.8212\nEpoch 21/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.6796 - acc: 0.9104 - val_loss: 1.3353 - val_acc: 0.8233\nEpoch 22/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.6736 - acc: 0.9115 - val_loss: 1.3397 - val_acc: 0.8217\nEpoch 23/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.6691 - acc: 0.9125 - val_loss: 1.3346 - val_acc: 0.8236\nEpoch 24/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.6638 - acc: 0.9131 - val_loss: 1.3380 - val_acc: 0.8244\nEpoch 25/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.6568 - acc: 0.9142 - val_loss: 1.3360 - val_acc: 0.8231\nEpoch 26/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.6430 - acc: 0.9155 - val_loss: 1.3298 - val_acc: 0.8226\nEpoch 27/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.6361 - acc: 0.9162 - val_loss: 1.3314 - val_acc: 0.8232\nEpoch 28/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.6341 - acc: 0.9173 - val_loss: 1.3347 - val_acc: 0.8236\nEpoch 29/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.6326 - acc: 0.9174 - val_loss: 1.3388 - val_acc: 0.8232\nEpoch 30/50\n938/938 [==============================] - 19s 20ms/step - loss: 0.6284 - acc: 0.9181 - val_loss: 1.3411 - val_acc: 0.8237\nEpoch 31/50\n938/938 [==============================] - 18s 20ms/step - loss: 0.6248 - acc: 0.9187 - val_loss: 1.3383 - val_acc: 0.8214\nEpoch 32/50\n938/938 [==============================] - 19s 20ms/step - loss: 0.6206 - acc: 0.9193 - val_loss: 1.3433 - val_acc: 0.8229\nEpoch 33/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.6157 - acc: 0.9200 - val_loss: 1.3435 - val_acc: 0.8226\nEpoch 34/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.6120 - acc: 0.9201 - val_loss: 1.3440 - val_acc: 0.8221\nEpoch 35/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.6079 - acc: 0.9206 - val_loss: 1.3433 - val_acc: 0.8220\nEpoch 36/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.6036 - acc: 0.9213 - val_loss: 1.3463 - val_acc: 0.8220\nEpoch 37/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.5990 - acc: 0.9219 - val_loss: 1.3491 - val_acc: 0.8211\nEpoch 38/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.5952 - acc: 0.9220 - val_loss: 1.3464 - val_acc: 0.8211\nEpoch 39/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.5915 - acc: 0.9224 - val_loss: 1.3440 - val_acc: 0.8225\nEpoch 40/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.5872 - acc: 0.9232 - val_loss: 1.3514 - val_acc: 0.8209\nEpoch 41/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.5837 - acc: 0.9234 - val_loss: 1.3475 - val_acc: 0.8228\nEpoch 42/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.5808 - acc: 0.9238 - val_loss: 1.3514 - val_acc: 0.8228\nEpoch 43/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.5771 - acc: 0.9239 - val_loss: 1.3491 - val_acc: 0.8215\nEpoch 44/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.5743 - acc: 0.9242 - val_loss: 1.3623 - val_acc: 0.8205\nEpoch 45/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.5719 - acc: 0.9248 - val_loss: 1.3573 - val_acc: 0.8218\nEpoch 46/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.5699 - acc: 0.9248 - val_loss: 1.3625 - val_acc: 0.8216\nEpoch 47/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.5680 - acc: 0.9251 - val_loss: 1.3644 - val_acc: 0.8210\nEpoch 48/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.5668 - acc: 0.9255 - val_loss: 1.3706 - val_acc: 0.8215\nEpoch 49/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.5640 - acc: 0.9256 - val_loss: 1.3698 - val_acc: 0.8217\nEpoch 50/50\n938/938 [==============================] - 16s 17ms/step - loss: 0.5616 - acc: 0.9258 - val_loss: 1.3777 - val_acc: 0.8207\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f61e843bd90>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# # model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "# #           validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "# #           batch_size=128, epochs=50, verbose = 2)\n",
    "\n",
    "# 메모리 터져서 batch_size 32\n",
    "model.fit([encoder_input_train, decoder_input_train],decoder_target_train,\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 32, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구현(테스트)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코더 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-22T09:43:41.329Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"functional_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, None)]            0         \n_________________________________________________________________\nembedding (Embedding)        (None, None, 256)         1227776   \n_________________________________________________________________\nmasking (Masking)            (None, None, 256)         0         \n_________________________________________________________________\nlstm (LSTM)                  [(None, 256), (None, 256) 525312    \n=================================================================\nTotal params: 1,753,088\nTrainable params: 1,753,088\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 디코더 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-22T09:43:41.331Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[<tf.Tensor 'input_3:0' shape=(None, 256) dtype=float32>,\n <tf.Tensor 'input_4:0' shape=(None, 256) dtype=float32>]"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "# # 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_states_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-22T09:43:41.333Z"
    }
   },
   "outputs": [],
   "source": [
    "dec_emb_test = Embedding(fra_vocab_size, dec_emb_dim)(decoder_inputs)\n",
    "decoder_outputs_test, state_h_test, state_c_test = decoder_lstm(dec_emb_test, initial_state = decoder_states_inputs)\n",
    "\n",
    "decoder_states_test = [state_h_test, state_c_test]\n",
    "\n",
    "decoder_outputs_test = decoder_softmax_layer(decoder_outputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-22T09:43:41.336Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"functional_5\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\nembedding_2 (Embedding)         (None, None, 256)    2312704     input_2[0][0]                    \n__________________________________________________________________________________________________\ninput_3 (InputLayer)            [(None, 256)]        0                                            \n__________________________________________________________________________________________________\ninput_4 (InputLayer)            [(None, 256)]        0                                            \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_2[0][0]                \n                                                                 input_3[0][0]                    \n                                                                 input_4[0][0]                    \n__________________________________________________________________________________________________\ndense (Dense)                   (None, None, 9034)   2321738     lstm_1[1][0]                     \n==================================================================================================\nTotal params: 5,159,754\nTrainable params: 5,159,754\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs_test] + decoder_states_test)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-22T09:43:41.338Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word\n",
    "\n",
    "# print(idx2fra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "fra2idx['\\t']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측 과정을 위한 decode_sequence() 함수 구현\n",
    "\n",
    "decode_sequence() 내부에는 인코더를 구현한 encoder_model이 있어서 이 모델에 번역하고자 하는 문장의 정수 시퀀스인 'input_seq'를 입력하면, encoder_model은 마지막 시점의 hidden state를 리턴합니다.\n",
    "\n",
    "이 hidden state는 디코더의 첫번째 시점의 hidden state가 되고, 디코더는 이제 번역 문장을 완성하기 위한 예측 과정을 진행합니다. 디코더의 예측 과정에서는 이전 시점에서 예측한 단어를 디코더의 현재 시점의 입력으로 넣어주는 작업을 진행합니다. 그리고 이 작업은 종료를 의미하는 종료 토큰을 만나거나, 주어진 최대 길이를 넘을 때까지 반복합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-22T09:43:41.341Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = fra2idx['\\t']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # print(states_value[0])\n",
    "    # print(target_seq)\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        # print(output_tokens)\n",
    "        # print(output_tokens.shape)\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        # print(sampled_token_index)\n",
    "        # sampled_token_index+=1\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가 ( 띄어쓰기좀 해주자)\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-22T09:43:41.343Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-----------------------------------\n입력 문장: run!\n정답 문장: ourez !\n번역기가 번역한 문장 길이 : 16\n번역기가 번역한 문장:  je je à à à à \n-----------------------------------\n입력 문장: i left.\n정답 문장: e suis partie\n번역기가 번역한 문장 길이 : 19\n번역기가 번역한 문장:  je me . . . parti\n-----------------------------------\n입력 문장: call us.\n정답 문장: ppelez-nous !\n번역기가 번역한 문장 길이 : 17\n번역기가 번역한 문장:  je . . . . . . \n-----------------------------------\n입력 문장: how nice!\n정답 문장: omme c'est ge\n번역기가 번역한 문장 길이 : 16\n번역기가 번역한 문장:  j'étais été ét\n-----------------------------------\n입력 문장: turn left.\n정답 문장: ourne à gauch\n번역기가 번역한 문장 길이 : 18\n번역기가 번역한 문장:  c'est vraiment t\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [4,50,100,300,1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "    # print(input_seq.shape)\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(decoder_input_test[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장 길이 :',len(decoded_sentence))\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "source": [
    "# 결과\n",
    "프랑스어를 할 줄 모르는 내가 보기에도 굉장히 말도 안되는 결과물들이 나오고 있다.\n",
    "decoder 모델 설계에 문제가 있는 것으로 생각된다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}