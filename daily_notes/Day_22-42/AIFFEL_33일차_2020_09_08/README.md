# AIFFEL_33ì¼ì°¨ 2020.09.08

Tags: AIFFEL_DAILY

### ì¼ì •

---

1. Dacon ì‘ì—… â‡’ 4ì‹œ íšŒì˜
2. LMS F-30
3. LMS E-11
4. ìˆ˜ìš”ì¼ í’€ì ì¤€ë¹„ (LSTM ë°œí‘œ ëª» ëë‚¸ ë¶€ë¶„ ë³µê¸° ë° ë‚´ìš© ì¶”ê°€,ì •ë¦¬)

# Dacon ì‘ì—…

---

1. ~~bagging ì´í›„ì— í‰ê· ìœ¼ë¡œ êµ¬í˜„í•˜ì§€ ì•Šê³ ... ë‹¤ë¥¸ ë°©ë²•ì„ ì‚¬ìš©í•´ë³´ì•„ì•¼ í•  ê²ƒ ê°™ì€ë°?~~

~~â‡’ ê°€ì¥ ë§ì´ ì˜ˆì¸¡ëœ ê²°ê³¼ë¥¼ ê³ ë¥´ëŠ”ê±¸ë¡œ.~~

2. train ë°ì´í„° ì„ì˜ ì¶”ê°€ëŠ” ì–´ë–¨ê¹Œ?  ì¢Œìš°ë°˜ì „í•œ ë°ì´í„°ë¥¼ í•œ ì„¸íŠ¸ì”© ë„£ì–´ì£¼ëŠ”ê±´?

- ìˆ«ìë§Œ ë¶„ë¦¬í•œ ê²°ê³¼ë¥¼ ì¢Œìš°ë°˜ì „í•˜ê³ 
- ë¬¸ìë§Œ ë¶„ë¦¬í•œ ê²°ê³¼ë¥¼ ì¢Œìš°ë°˜ì „í•˜ê³ 
- ì›ë³¸ë„ ì¢Œìš°ë°˜ì „í•˜ê³ 
- ê²°ê³¼ì ìœ¼ë¡œ train ë°ì´í„°ê°€ 2ë°°ê°€ ë˜ë„ë¡ í•´ë³¸ë‹¤.

~~3. ì¼ë‹¨ ê¸°ì¡´ 3ì±„ë„ ë°ì´í„°ë¡œ bagging í•´ì„œ ê²°ê³¼ ë§Œë“¤ì–´ë³´ê³ ~~

~~4. train ë°ì´í„° ì„ì˜ ì¶”ê°€í•œ ë°ì´í„°ë¡œ bagging í•´ì„œ ê²°ê³¼ ë§Œë“¤ì–´ë³´ì.~~

5. ê·¸ëƒ¥ ì¢Œìš°ë°˜ì „ ë°ì´í„° í•œ ì„¸íŠ¸ì”© ë„£ì–´ë³´ì.

6. ê·¸ë¦¬ê³  ì¢Œìš°ë°˜ì „ ë°ì´í„°ë¥¼ í†µí•´ ë§Œë“¤ì–´ì§„ ë°ì´í„°ë¥¼ LGBMìœ¼ë¡œ í•™ìŠµ ì‹œí‚¬ ë•Œ baggingìœ¼ë¡œ ì•™ìƒë¸”?!

# [F-30] ë”¥ëŸ¬ë‹ ë ˆì´ì–´ì˜ ì´í•´ (1)Linear, Convolution

---

## **í•™ìŠµ ëª©í‘œ**

---

1. ë ˆì´ì–´ì˜ ê°œë…ì„ ì´í•´í•œë‹¤.
2. ë”¥ëŸ¬ë‹ ëª¨ë¸ ì† ê° ë ˆì´ì–´(Linear, Convolution)ì˜ ë™ì‘ ë°©ì‹ì„ ì´í•´í•œë‹¤.
3. ë°ì´í„°ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë ˆì´ì–´ë¥¼ ì„¤ê³„í•˜ê³ , ì´ë¥¼Â *Tensorflow*ë¡œ ì •ì˜í•˜ëŠ” ë²•ì„ ë°°ìš´ë‹¤.

***ë¯¸ì‹± ë§í¬(missing link)***

### ë°ì´í„°ì˜ í˜•íƒœ

---

(1920, 1080, 3) ì˜ ë§¤íŠ¸ë¦­ìŠ¤ (W, H, C) ì™€ ê°™ì´ í‘œê¸° (C : Channel)

### ë ˆì´ì–´?

---

í•˜ë‚˜ì˜ ë¬¼ì²´ê°€ ì—¬ëŸ¬ê°œì˜ ë…¼ë¦¬ì ì¸ ê°ì²´ë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ” ê²½ìš°, ì´ëŸ¬í•œ ê°ê°ì˜ ê°ì²´ë¥¼ í•˜ë‚˜ì˜ ë ˆì´ì–´ë¼ í•œë‹¤.

ì‹ ê²½ë§ì´ë¼ëŠ” ë¬¼ì²´ë¥¼ êµ¬ì„±í•˜ëŠ” ì—¬ëŸ¬ ê°œì˜ ë…¼ë¦¬ì ì¸ ë ˆì´ì–´ë“¤ì„ ì´í•´í•˜ëŠ” ê²ƒì€ ê³§ ì‹ ê²½ë§ ê·¸ ìì²´ë¥¼ ì´í•´í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. Weight ë¼ëŠ” ê²ƒì€ ì •í™•íˆëŠ” ë ˆì´ì–´ì˜ Weight! ì‹ ê²½ë§ì€ ë ˆì´ì–´ë“¤ì˜ ê°ê¸° ë‹¤ë¥¸ Weight, ê·¸ê²ƒë“¤ì´ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ì´ë¤„ë‚´ëŠ” í•˜ë‚˜ì˜ ê²°ê³¼ë¬¼.

## Linear ë ˆì´ì–´

---

Fully Connected Layer, Feedforward Neural Network, Multilayer Perceptrons, Dense Layerâ€¦ ë“± ë‹¤ì–‘í•œ ì´ë¦„ìœ¼ë¡œ ë¶ˆë¦¬ì§€ë§Œ ê·¸ ëª¨ë“  ê²ƒë“¤ì€ ê²°êµ­ Linear ë ˆì´ì–´ì— í•´ë‹¹í•˜ë©° ê·¸ëŸ° ì´ìœ ì—ì„œ í•„ìëŠ” Linear ë ˆì´ì–´ë¼ê³  ì¹­í•˜ëŠ” ê²ƒì„ ì„ í˜¸í•©ë‹ˆë‹¤.

ì„ í˜•ëŒ€ìˆ˜í•™ì—ì„œ ì“°ì´ëŠ” ìš©ì–´ ì¤‘ **ì„ í˜• ë³€í™˜(Linear Transform)**ì´ ìˆëŠ”ë°, **ê·¸ê²ƒê³¼ ì™„ì „íˆ ë™ì¼í•œ ê¸°ëŠ¥**ì„ í•˜ëŠ” ë ˆì´ì–´ì…ë‹ˆë‹¤.

### ì„ í˜• ë³€í™˜, ì§ê´€ì ì¸ ì´í•´

---

[https://www.youtube.com/watch?v=vVvjYzFBUVk&feature=youtu.be&ab_channel=ìˆ˜í•™ë…¸íŠ¸ìƒìš°ìŒ¤ì˜](https://www.youtube.com/watch?v=vVvjYzFBUVk&feature=youtu.be&ab_channel=%EC%88%98%ED%95%99%EB%85%B8%ED%8A%B8%EC%83%81%EC%9A%B0%EC%8C%A4%EC%9D%98)

í–‰ë ¬ì„ ì´ìš©í•˜ì—¬ ê¸°ì¡´ì˜ ì¢Œí‘œì—ì„œ ìƒˆë¡œìš´ ì¢Œí‘œë¥¼ ë§Œë“ ë‹¤.

ëª¨ë“  ì ë“¤ì— ë™ì¼í•œ í–‰ë ¬ì„ ì ìš©í•˜ë©´ í™•ëŒ€ë‚˜ ì¶•ì†Œ ë“±ì„ í•  ìˆ˜ ìˆë‹¤.

ìƒˆë¡œìš´ ì¢Œí‘œë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” í•¨ìˆ˜ì™€ ê°™ì€ ê²ƒ.

í–‰ë ¬ì— ì˜í•´ ë³€í˜•ëœ ê³µê°„ì—ì„œ ì§ì„ ë“¤ì„ í‰í–‰ì„ ìœ ì§€í•˜ê²Œ ë˜ëŠ”ë°, ì´ê²ƒì€ ìœ í´ë¦¬ë“œê¸°í•˜ì—ì„œ ë‹¤ë£¨ëŠ” ê¸°í•˜í•™ì  êµ¬ì¡°. ì„ í˜•ëŒ€ìˆ˜ë€ í–‰ë ¬ì„ ì´ìš©í•˜ì—¬ ì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ ëŒ€ìˆ˜ì ìœ¼ë¡œ ê³„ì‚°í•˜ê³  ë¶„ì„í•˜ëŠ” í•™ë¬¸

### ì„ í˜• ë³€í™˜, ìì„¸í•œ ì„¤ëª…

---

[https://www.youtube.com/watch?v=kYB8IZa5AuE&feature=youtu.be&ab_channel=3Blue1Brown](https://www.youtube.com/watch?v=kYB8IZa5AuE&feature=youtu.be&ab_channel=3Blue1Brown)

Linear ë ˆì´ì–´ëŠ” ì„ í˜• ë³€í™˜ì„ í™œìš©í•´ ë°ì´í„°ë¥¼ íŠ¹ì • ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ëŠ¥ì„ í•©ë‹ˆë‹¤.

[ex]

![images/Untitled.png](images/Untitled.png)

ìœ„ ê·¸ë¦¼ì˜ ë‘ ì‚¬ê°í˜•ì€ ëª¨ë‘ (x, y) 2ì°¨ì›ì˜ ì  4ê°œë¡œ í‘œí˜„ ê°€ëŠ¥í•˜ë¯€ë¡œ, ê°ê° (4, 2) í˜•íƒœì˜ ë°ì´í„°ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‘ ì‚¬ê°í˜•ì„ ê°ê° ì–´ë–¤ í•˜ë‚˜ì˜ ì •ìˆ˜ë¡œ í‘œí˜„í•˜ê³ ì í•©ë‹ˆë‹¤. ì‹¤ì€ ì´ ì •ìˆ˜ëŠ” ìš°ë¦¬ê°€ êµ¬ë¶„í•˜ê³ ì í•˜ëŠ” ì‚¬ê°í˜•ì˜ ì¢…ë¥˜(class)ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.ì´ë¥¼ ìœ„í•´, ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì§‘ì•½ì‹œí‚¬ì§€ êµ¬ìƒí•´ë³´ê² ìŠµë‹ˆë‹¤.

### ë°ì´í„° ì§‘ì•½

---

***<ì‹1>***

*1ë‹¨ê³„: **(4, 2) x [2 x 1 í–‰ë ¬] = (4, )***

*2ë‹¨ê³„: **(4, ) x [4 x 1 í–‰ë ¬] = (1, )***

ìœ„ ë‹¨ê³„ë¥¼ ì‚¬ìš©í•˜ë©´ ê°ê°ì˜ ì‚¬ê°í˜•ì„, ì •ë³´ê°€ ì§‘ì•½ëœ í•˜ë‚˜ì˜ ì •ìˆ˜ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 2ì°¨ì›ì„ 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë°ì— 2 x 1 í–‰ë ¬ì´ í•˜ë‚˜ ì„ ì–¸ë˜ê³ , 4ì°¨ì›ì„ 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë°ì— 4 x 1 í–‰ë ¬ì´ í•˜ë‚˜ ì„ ì–¸ë¨ì— ìœ ì˜í•©ì‹œë‹¤.

ì—¬ê¸°ì„œ ê°ê°ì˜ í–‰ë ¬ë“¤ì´ Weightì…ë‹ˆë‹¤. Linear ë ˆì´ì–´ëŠ” (ì…ë ¥ì˜ ì°¨ì›, ì¶œë ¥ì˜ ì°¨ì›)ì— í•´ë‹¹í•˜ëŠ” Weightë¥¼ ê°€ì§€ëŠ” íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

![images/Untitled%201.png](images/Untitled%201.png)

ë‘ ì‚¬ê°í˜•ì— ëŒ€í•´ 1ë‹¨ê³„ë¥¼ ê±°ì¹˜ê³  ë‚œ ê²°ê³¼ê°€ ë™ì¼í•˜êµ°ìš”.

ë ‡ê²Œ ë˜ë©´ <ì‹ 1>ì˜ 2ë‹¨ê³„ ì…ë ¥ì´ ë™ì¼í•´ì§€ë‹ˆ ë‘ ë²ˆì§¸ 4 x 1 Weightë¥¼ ê±°ì¹˜ëŠ” ê²ƒì´ ì˜ë¯¸ê°€ ì—†ì–´ì§‘ë‹ˆë‹¤.

ì—¬ê¸°ì„œ ëª¨ë“  Weightì˜ ëª¨ë“  ìš”ì†Œë¥¼ ***Parameter***ë¼ê³  í•©ë‹ˆë‹¤.

ì´ 6ê°œ (ìœ„ ê·¸ë¦¼ì—ì„œëŠ” 2ê°œ)ì˜ Parameterë¡œ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸°ì—” ì—­ë¶€ì¡±ì´ì—ˆë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤

```python
import tensorflow as tf

batch_size = 64
boxes = tf.zeros((batch_size, 4, 2))     # TensorflowëŠ” Batchë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•˜ê¸°ì—,
                                         # ìš°ë¦¬ëŠ” ì‚¬ê°í˜• 2ê°œ ì„¸íŠ¸ë¥¼ batch_sizeê°œë§Œí¼
                                         # ë§Œë“  í›„ ì²˜ë¦¬ë¥¼ í•˜ê²Œ ë©ë‹ˆë‹¤.
print("1ë‹¨ê³„ ì—°ì‚° ì¤€ë¹„:", boxes.shape)

first_linear = tf.keras.layers.Dense(units=1, use_bias=False) 
# unitsì€ ì¶œë ¥ ì°¨ì› ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
# Weight í–‰ë ¬ ì† ì‹¤ìˆ˜ë¥¼ ì¸ê°„ì˜ ë‡Œ ì† í•˜ë‚˜ì˜ ë‰´ëŸ° 'ìœ ë‹›' ì·¨ê¸‰ì„ í•˜ëŠ” ê±°ì£ !

first_out = first_linear(boxes)
first_out = tf.squeeze(first_out, axis=-1) # (4, 1)ì„ (4,)ë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤.
                                           # (ë¶ˆí•„ìš”í•œ ì°¨ì› ì¶•ì†Œ)

print("1ë‹¨ê³„ ì—°ì‚° ê²°ê³¼:", first_out.shape)
print("1ë‹¨ê³„ Linear Layerì˜ Weight í˜•íƒœ:", first_linear.weights[0].shape)

print("\n2ë‹¨ê³„ ì—°ì‚° ì¤€ë¹„:", first_out.shape)

second_linear = tf.keras.layers.Dense(units=1, use_bias=False)
second_out = second_linear(first_out)
second_out = tf.squeeze(second_out, axis=-1)

print("2ë‹¨ê³„ ì—°ì‚° ê²°ê³¼:", second_out.shape)
print("2ë‹¨ê³„ Linear Layerì˜ Weight í˜•íƒœ:", second_linear.weights[0].shape)
```

### ë°ì´í„°ë¥¼ ë” í’ë¶€í•˜ê²Œ

---

***<ì‹2>***

*1ë‹¨ê³„: **(4, 2) x [2 x 3 í–‰ë ¬] = (4, 3)***

*2ë‹¨ê³„: **(4, 3) x [3 x 1 í–‰ë ¬] = (4, )***

*3ë‹¨ê³„: **(4, ) x [4 x 1 í–‰ë ¬] = (1, )***

![images/Untitled%202.png](images/Untitled%202.png)

â‡’ 1ë‹¨ê³„ì˜ ê²°ê³¼ë¡œ ê° ì‚¬ê°í˜•ì— ëŒ€í•´ ë…ë¦½ì ì¸ ì •ë³´ê°€ ìƒê²¨ë‚˜ê¸° ì‹œì‘

```python
import tensorflow as tf

batch_size = 64
boxes = tf.zeros((batch_size, 4, 2))     # TensorflowëŠ” Batchë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•˜ê¸°ì—,
                                         # ìš°ë¦¬ëŠ” ì‚¬ê°í˜• 2ê°œ ì„¸íŠ¸ë¥¼ batch_sizeê°œë§Œí¼
                                         # ë§Œë“  í›„ ì²˜ë¦¬ë¥¼ í•˜ê²Œ ë©ë‹ˆë‹¤.
print("1ë‹¨ê³„ ì—°ì‚° ì¤€ë¹„:", boxes.shape)

first_linear = tf.keras.layers.Dense(units=3, use_bias=False)
first_out = first_linear(boxes)

print("1ë‹¨ê³„ ì—°ì‚° ê²°ê³¼:", first_out.shape)
print("1ë‹¨ê³„ Linear Layerì˜ Weight í˜•íƒœ:", first_linear.weights[0].shape)

print("\n2ë‹¨ê³„ ì—°ì‚° ì¤€ë¹„:", first_out.shape)

second_linear = tf.keras.layers.Dense(units=1, use_bias=False)
second_out = second_linear(first_out)
second_out = tf.squeeze(second_out, axis=-1)

print("2ë‹¨ê³„ ì—°ì‚° ê²°ê³¼:", second_out.shape)
print("2ë‹¨ê³„ Linear Layerì˜ Weight í˜•íƒœ:", second_linear.weights[0].shape)

print("\n3ë‹¨ê³„ ì—°ì‚° ì¤€ë¹„:", second_out.shape)

third_linear = tf.keras.layers.Dense(units=1, use_bias=False)
third_out = third_linear(second_out)
third_out = tf.squeeze(third_out, axis=-1)

print("3ë‹¨ê³„ ì—°ì‚° ê²°ê³¼:", third_out.shape)
print("3ë‹¨ê³„ Linear Layerì˜ Weight í˜•íƒœ:", third_linear.weights[0].shape)

total_params = \
first_linear.count_params() + \
second_linear.count_params() + \
third_linear.count_params()

print("ì´ Parameters:", total_params)
```

paramì´ ì§€ë‚˜ì¹˜ê²Œ ë§ë‹¤ë©´, ì •ë‹µë§Œ ì™¸ìš´ í•™ìƒê³¼ ê°™ì€ ê²ƒ! ì‹¤ì œ ì‹œí—˜ì—ì„œëŠ” ì¢‹ì€ ì„±ì ì„ ê±°ë‘ì§€ ëª»í•œë‹¤.

### í¸í–¥(Bias)

---

![images/Untitled%203.png](images/Untitled%203.png)

ë‘ ë°ì´í„°ê°€ ë¹„ìŠ·í•˜ê²Œ ìƒê²¼ì§€ë§Œ, ì›ì ì„ ê±´ë“¤ì§€ ì•Šê³  ë‘˜ì„ ì¼ì¹˜ì‹œí‚¤ê¸°ëŠ” ì–´ë ¤ì›Œ ë³´ì´ì£ ?

í¸í–¥ì´ ì—†ë‹¤ë©´ íŒŒë¼ë¯¸í„°ë¥¼ ì•„ë¬´ë¦¬ ëŒë¦¬ê³  ëŠ˜ë¦¬ê³  í•´ë„ ì •í™•í•˜ê²Œ ê·¼ì‚¬í•  ìˆ˜ ì—†ìŒì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆ.

ë‹¨ìˆœíˆ ìƒê°í•´ì„œ ì›ì ì„ í‰í–‰ì´ë™í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ í•´ê²°í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œ í¸í–¥ì€ ì„ í˜•ë³€í™˜ëœ ê°’ì— í¸í–¥ íŒŒë¼ë¯¸í„° bë¥¼ ë”í•´ì£¼ëŠ” ê²ƒìœ¼ë¡œ í‘œí˜„í•´ìš”.

## Convolution Layer : ì •ë³´ë¥¼ ì§‘ì•½ì‹œí‚¤ì

---

![images/Untitled%204.png](images/Untitled%204.png)

convolution ì—°ì‚° ì˜ˆì‹œ

ì§„ì„ ì„ ëª…í•˜ê²Œ í•˜ëŠ” í•„í„°ì™€ íë¦¬ê²Œ í•˜ëŠ” í•„í„° ë“± ë‹¤ì–‘í•œ í•„í„°ë“¤ì´ Convolutionì„ ìœ„í•œ í–‰ë ¬ë¡œ ì •ì˜ë˜ì–´ ìˆë‹µë‹ˆë‹¤ ğŸ˜‰

***í•„í„°ëŠ” ë‹¤ë¥¸ ë§ë¡œ ì»¤ë„***ì´ë¼ê³  ë¶€ë¥´ê¸°ë„ í•´ìš”! ì´ë¯¸ì§€ë¥¼ í•„í„°ë¡œ í›‘ì„ ë•Œ, í•œ ì¹¸ì”© ì´ë™í•˜ë©° í›‘ì„ ìˆ˜ë„ ìˆì§€ë§Œ, **ë‘ ì¹¸, ì„¸ ì¹¸ì”© ì´ë™í•˜ë©° í›‘ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤**. ê·¸ê²ƒì„ **ê²°ì •í•˜ëŠ” ê°’ì„ Stride**ë¼ê³  ì¹­í•©ë‹ˆë‹¤.

Convolution ì—°ì‚°ì€ ì…ë ¥ì˜ í˜•íƒœë¥¼ ë³€í˜•ì‹œí‚¨ë‹¤

â‡’ ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ ê°œë…ì´ Padding

í•„í„°ê°€ ì–´ë–¤ ëª©ì ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤ë©´**, ì†ì„ ì°¾ëŠ” ë°ì— ì í•©í•œ í•„í„°**ë„ ì¡´ì¬í•  ìˆ˜ ìˆì§€ ì•Šì„ê¹Œìš”?

ëª©ì ì— ë„ì›€ì´ ë˜ëŠ” ì •ë³´ëŠ” ì„ ëª…í•˜ê²Œ, ê·¸ë ‡ì§€ ì•Šì€ ì •ë³´ëŠ” íë¦¬ê²Œ ë§Œë“œëŠ” í•„í„°ë¥¼ ìƒìƒí•  ìˆ˜ ìˆê² êµ°ìš”!

ê·¸ëŸ° ë©‹ì§„ í•„í„°ë¥¼ í›ˆë ¨ì„ í†µí•´ ì°¾ì•„ì£¼ëŠ” ê²ƒì´ ë°”ë¡œ Convolution ë ˆì´ì–´ê°€ í•˜ëŠ” ì¼ì…ë‹ˆë‹¤.

ëª©ì ì— ë§ëŠ” í•„í„°ë¥¼ í›ˆë ¨ì„ í†µí•´ ì°¾ì•„ì£¼ëŠ” ê²ƒì´ ë°”ë¡œ Convolution ë ˆì´ì–´ê°€ í•˜ëŠ” ì¼

ì‹¬ì§€ì–´ëŠ” ë‹¨ í•˜ë‚˜ì˜ í•„í„°ê°€ ì•„ë‹Œ **ìˆ˜ì‹­ ê°œì˜ í•„í„°**ë¥¼ ì¤‘ì²©í•´ì„œìš”

![images/Untitled%205.png](images/Untitled%205.png)

***<ì‹ 4>***

*1ë‹¨ê³„: **(1920, 1080, 3) x [3 x 16 x 5 x 5 Weight & Stride 5] = (384, 216, 16)***

*2ë‹¨ê³„: **(384, 216, 16) â†’ (384 x 216 x 16, )***

*3ë‹¨ê³„: **(1327104, ) x [1327104 x 1 Weight] = (1, )***

Q. ìœ„ì˜ ê·¸ë¦¼ê³¼ ì‹ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œë¥¼ ì°¾ëŠ”ë‹¤ë©´? (í•„í„°ì™€ ê´€ë ¨í•˜ì—¬)

---

í•„í„°ì˜ í¬ê¸°ê°€ ì†(ëŒ€ìƒ)ì„ íŒë³„í•˜ê¸°ì—ëŠ” ì‘ì„ ìˆ˜ ìˆê³ , í•„í„°ì˜ í¬ê¸°ì™€ strideê°€ ê°™ìœ¼ë©´ ì†(ëŒ€ìƒ)ì´ í•„í„°ì˜ ê²½ê³„ì„ ì— ê±¸ë¦¬ëŠ” ê²½ìš°, ì •ìƒì ìœ¼ë¡œ ì¸ì‹í•˜ì§€ ëª»í•  ìˆ˜ ìˆë‹¤. (strideë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ì¢‹ì•„ë³´ì¸ë‹¤.)

```python
import tensorflow as tf

batch_size = 64
pic = tf.zeros((batch_size, 1920, 1080, 3))

print("ì…ë ¥ ì´ë¯¸ì§€ ë°ì´í„°:", pic.shape)
conv_layer = tf.keras.layers.Conv2D(filters=16,
                                    kernel_size=(5, 5),
                                    strides=5,
                                    use_bias=False)
conv_out = conv_layer(pic)

print("\nConvolution ê²°ê³¼:", conv_out.shape)
print("Convolution Layerì˜ Parameter ìˆ˜:", conv_layer.count_params())

flatten_out = tf.keras.layers.Flatten()(conv_out)
print("\n1ì°¨ì›ìœ¼ë¡œ í¼ì¹œ ë°ì´í„°:", flatten_out.shape)

linear_layer = tf.keras.layers.Dense(units=1, use_bias=False)
linear_out = linear_layer(flatten_out)

print("\nLinear ê²°ê³¼:", linear_out.shape)
print("Linear Layerì˜ Parameter ìˆ˜:", linear_layer.count_params())
```

Linear ë ˆì´ì–´ëŠ” ì…ë ¥ í”¼ì²˜ ì „ì²´ê°€ ë§¤ ì¶œë ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì˜ ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë“  ì…ë ¥ í”¼ì²˜ ì‚¬ì´ì— ì „ë¶€ ê³ ë ¤í•©ë‹ˆë‹¤.
ì´ë¯¸ì§€ì²˜ëŸ¼ ì§€ì—­ì„±(Locality) ê·¸ ìì²´ê°€ ì—„ì²­ë‚˜ê²Œ ì¤‘ìš”í•œ ì •ë³´ê°€ ë˜ëŠ” ê²½ìš°, Linear ë ˆì´ì–´ëŠ” ê·¸ ì¤‘ìš”í•œ ì •ë³´ê°€ ëª¨ë‘ ì†Œì‹¤ëœ ì±„ ì—„ì²­ë‚˜ê²Œ í° íŒŒë¼ë¯¸í„° ì†ì—ì„œ ì…ë ¥ê³¼ ì¶œë ¥ ì‚¬ì´ì˜ ê´€ê³„ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ì•„ë‚´ì•¼ í•˜ëŠ” ì–´ë ¤ìš´ ë¬¸ì œë¥¼ í’€ì–´ì•¼ í•©ë‹ˆë‹¤.
ê·¸ëŸ¬ë‚˜ Convolution ë ˆì´ì–´ëŠ” í•„í„° êµ¬ì¡° ì•ˆì— Locality ì •ë³´ê°€ ì˜¨ì „íˆ ë³´ì¡´ë©ë‹ˆë‹¤. ì¸ì ‘í•œ í”½ì…€ë“¤ ì‚¬ì´ì—ì„œì˜ íŒ¨í„´ë§Œ ì¶”ì¶œí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ ìì²´ë§Œìœ¼ë¡œë„ ë¶ˆí•„ìš”í•œ íŒŒë¼ë¯¸í„° ë° ì—°ì‚°ëŸ‰ì„ ì œê±°í•˜ê³  í›¨ì”¬ ì •í™•í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì •ë³´ë¥¼ ì§‘ì•½ì‹œí‚¬ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

## Pooling ë ˆì´ì–´ : í•µì‹¬ë§Œ ì¶”ë ¤ì„œ ë” ë„“ê²Œ!

---

ë§Œì•½ ê·¹ë‹¨ì ìœ¼ë¡œ í•„í„° ì‚¬ì´ì¦ˆë¥¼ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆì™€ ë™ì¼í•˜ê²Œ í•œë‹¤ë©´ ì–´ë–¤ ì¼ì´ ìƒê¸¸ê¹Œìš”?

ê·¸ ìˆœê°„ ìš°ë¦¬ì˜ Convolution ë ˆì´ì–´ëŠ” ì™„ë²½í•˜ê²Œ Linear ë ˆì´ì–´ì™€ ê°™ì•„ì§€ê²Œ ë©ë‹ˆë‹¤.

í•„í„° ì‚¬ì´ì¦ˆë¥¼ í‚¤ìš°ê²Œ ë˜ë©´ íŒŒë¼ë¯¸í„° ì‚¬ì´ì¦ˆì™€ ì—°ì‚°ëŸ‰ì´ ì»¤ì§ˆ ë¿ ì•„ë‹ˆë¼, Accuracyë„ ë–¨ì–´ì§€ê²Œ ë  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.

### Receptive Field

---

ì…ë ¥ ë°ì´í„°ì˜ Receptive Fieldê°€ ì¶©ë¶„íˆ ì»¤ì„œ ê·¸ ì•ˆì— detectí•´ì•¼ í•  ***objectì˜ íŠ¹ì„±ì´ ì¶©ë¶„íˆ í¬í•¨***ë˜ì–´ ìˆì–´ì•¼ ì •í™•í•œ detectionì´ ê°€ëŠ¥í•˜ê²Œ ë©ë‹ˆë‹¤.

### Max Pooling ë ˆì´ì–´ì˜ ì˜ë¯¸

---

Max Pooling ë ˆì´ì–´ë¥¼ í†µí•´ íš¨ê³¼ì ìœ¼ë¡œ Receptive Fieldë¥¼ í‚¤ìš°ê³ , ì •ë³´ ì§‘ì•½ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬ëŠ” ë™ì•ˆ ëŠ˜ì–´ë‚œ íŒŒë¼ë¯¸í„° ì‚¬ì´ì¦ˆëŠ” ì–¼ë§ˆì¼ê¹Œìš”? ë„¤ ì •í™•íˆ 0ì…ë‹ˆë‹¤.

**ê¸°ê» Convolution ë ˆì´ì–´ì—ì„œ í˜ë“¤ê²Œ ì—°ì‚°í•œ ê²°ê³¼ì˜ 3/4ë¥¼ ê·¸ëƒ¥ ë²„ë¦¬ëŠ” ê²ƒ**ì…ë‹ˆë‹¤. ì™œ ì´ëŸ° ë‚­ë¹„ë¥¼ í•˜ê²Œ ë˜ëŠ” ê²ƒì¼ê¹Œìš”? ê·¸ë¦¬ê³  **ì´ëŸ° ì •ë³´ ì†ì‹¤ì´ ê°€ì ¸ì˜¤ëŠ” Accuracy ì €í•˜ íš¨ê³¼ëŠ” ì—†ëŠ” ê²ƒì¼ê¹Œìš”?**

Q.ê·¸ë¦¬ê³  ì™œ ì´ê²Œ íš¨ê³¼ê°€ ìˆëŠ” ê²ƒì¸ê°€?? ëª…í™•í•œ ì„¤ëª…ì€ ì—†ì§€ë§Œ...ëª‡ ê°€ì§€ ì„¤ëª…ì´ ìˆê¸´ í•˜ë‹¤.

1. **translational invariance íš¨ê³¼**

ì´ë¯¸ì§€ëŠ” ì•½ê°„ì˜ ìƒí•˜ì¢Œìš° ì‹œí”„íŠ¸ê°€ ìƒê¸´ë‹¤ê³  í•´ë„ ë‚´ìš©ìƒ ë™ì¼í•œ íŠ¹ì§•ì´ ìˆëŠ”ë°, Max Poolingì„ í†µí•´ ì¸ì ‘í•œ ì˜ì—­ ì¤‘ ê°€ì¥ íŠ¹ì§•ì´ ë‘ë“œëŸ¬ì§„ ì˜ì—­ í•˜ë‚˜ë¥¼ ë½‘ëŠ” ê²ƒì€ ì˜¤íˆë ¤ ì•½ê°„ì˜ ì‹œí”„íŠ¸ íš¨ê³¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ë™ì¼í•œ íŠ¹ì§•ì„ ì•ˆì •ì ìœ¼ë¡œ ì¡ì•„ë‚¼ ìˆ˜ ìˆëŠ” ê¸ì •ì  íš¨ê³¼ê°€ ìˆì–´ì„œ ì˜¤íˆë ¤ object ìœ„ì¹˜ì— ëŒ€í•œ ì˜¤ë²„í”¼íŒ…ì„ ë°©ì§€í•˜ê³  ì•ˆì •ì ì¸ íŠ¹ì§• ì¶”ì¶œ íš¨ê³¼ë¥¼ ê°€ì ¸ì˜¨ë‹¤ê³  í•©ë‹ˆë‹¤.

2. **Non-linear í•¨ìˆ˜ì™€ ë™ì¼í•œ í”¼ì²˜ ì¶”ì¶œ íš¨ê³¼**

Reluì™€ ê°™ì€ **Non-linear í•¨ìˆ˜ë„ ë§ˆì°¬ê°€ì§€ë¡œ ë§ì€ í•˜ìœ„ ë ˆì´ì–´ì˜ ì—°ì‚° ê²°ê³¼ë¥¼ ë¬´ì‹œí•˜ëŠ” íš¨ê³¼ (0ë¯¸ë§Œ ë²„ë ¤!)** ë¥¼ ë°œìƒì‹œí‚¤ì§€ë§Œ, ê·¸ ê²°ê³¼ **ì¤‘ìš”í•œ í”¼ì²˜ë§Œì„ ìƒìœ„ ë ˆì´ì–´ë¡œ ì¶”ì¶œí•´ì„œ ì˜¬ë ¤ì¤Œ**ìœ¼ë¡œì¨ ê²°ê³¼ì ìœ¼ë¡œ ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ì„ ì¦ì§„ì‹œí‚¤ëŠ” íš¨ê³¼ë¥¼ ê°€ì§‘ë‹ˆë‹¤. Min/Max Poolingë„ ì´ì™€ ë™ì¼í•œ íš¨ê³¼ë¥¼ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤.

3. **Receptive Field ê·¹ëŒ€í™” íš¨ê³¼**

Max Poolingì´ ì—†ì´ë„ Receptive Fieldë¥¼ í¬ê²Œ í•˜ë ¤ë©´ Convolutional ë ˆì´ì–´ë¥¼ ì•„ì£¼ ë§ì´ ìŒ“ì•„ì•¼ í•©ë‹ˆë‹¤. ê·¸ ê²°ê³¼ í° íŒŒë¼ë¯¸í„° ì‚¬ì´ì¦ˆë¡œ ì¸í•œ ì˜¤ë²„í”¼íŒ…, ì—°ì‚°ëŸ‰ ì¦ê°€, Gradient Vanishing ë“±ì˜ ë¬¸ì œë¥¼ ê°ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŸ° ë¬¸ì œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ê¼½íˆëŠ” ë‘ê°€ì§€ ì¤‘ í•˜ë‚˜ê°€ Max Pooling ë ˆì´ì–´ ì‚¬ìš©ì…ë‹ˆë‹¤. ë‹¤ë¥¸ í•˜ë‚˜ë¡œëŠ” Dilated Convolutionì´ ìˆìŠµë‹ˆë‹¤.

(ì°¸ì¡° : [https://m.blog.naver.com/sogangori/220952339643](https://m.blog.naver.com/sogangori/220952339643))

## Deconvolution ë ˆì´ì–´ : ì§‘ì•½ëœ ì •ë³´ì˜ ë³µì›!

---

onvolutionì˜ ê²°ê³¼ë¥¼ ì—­ì¬ìƒí•´ì„œ ì›ë³¸ ì´ë¯¸ì§€ì™€ ìµœëŒ€í•œ ìœ ì‚¬í•œ ì •ë³´ë¥¼ ë³µì›í•´ ë‚´ëŠ” Auto Encoderì— ëŒ€í•´ ì•Œì•„ë³´ë ¤ê³  í•©ë‹ˆë‹¤.

```python
import numpy as np
from tensorflow.python.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.datasets import mnist
import json
import matplotlib.pyplot as plt #for plotting

# MNIST ë°ì´í„° ë¡œë”©
(x_train, _), (x_test, _) = mnist.load_data()    # y_train, y_testëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

x_train = np.expand_dims(x_train, axis=3)
x_test = np.expand_dims(x_test, axis=3)

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
```

ì–¼í• ë´ì„œëŠ” ê·¸ë™ì•ˆ ë§ì´ ìˆ˜í–‰í•´ ë³´ì…¨ì„ MNIST ë°ì´í„°ì…‹ ë¡œë”© ì ˆì°¨ì…ë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ ì˜ ë³´ì‹œë©´ y_train, y_testë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì™œì¼ê¹Œìš”? **AutoEncoderê°€ ìˆ˜í–‰í•˜ëŠ” Image Reconstruction TaskëŠ” x_trainì˜ ë¼ë²¨ì´ ë°”ë¡œ x_train ìì‹ ì´ ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.**

```python
# AutoEncoder ëª¨ë¸ êµ¬ì„± - Input ë¶€ë¶„
input_shape = x_train.shape[1:]
input_img = Input(shape=input_shape)

# AutoEncoder ëª¨ë¸ êµ¬ì„± - Encoder ë¶€ë¶„
encode_conv_layer_1 = Conv2D(16, (3, 3), activation='relu', padding='same')
encode_pool_layer_1 = MaxPooling2D((2, 2), padding='same')
encode_conv_layer_2 = Conv2D(8, (3, 3), activation='relu', padding='same')
encode_pool_layer_2 = MaxPooling2D((2, 2), padding='same')
encode_conv_layer_3 = Conv2D(4, (3, 3), activation='relu', padding='same')
encode_pool_layer_3 = MaxPooling2D((2, 2), padding='same')

encoded = encode_conv_layer_1(input_img)
encoded = encode_pool_layer_1(encoded)
encoded = encode_conv_layer_2(encoded)
encoded = encode_pool_layer_2(encoded)
encoded = encode_conv_layer_3(encoded)
encoded = encode_pool_layer_3(encoded)

# Encoder í†µê³¼ ì§í›„ì˜ Outputì€ 4x4ì˜ í˜•íƒœ. 

# AutoEncoder ëª¨ë¸ êµ¬ì„± - Decoder ë¶€ë¶„
decode_conv_layer_1 = Conv2D(4, (3, 3), activation='relu', padding='same')
decode_upsample_layer_1 = UpSampling2D((2, 2))
decode_conv_layer_2 = Conv2D(8, (3, 3), activation='relu', padding='same')
decode_upsample_layer_2 = UpSampling2D((2, 2))
decode_conv_layer_3 = Conv2D(16, (3, 3), activation='relu')
decode_upsample_layer_3 = UpSampling2D((2, 2))
decode_conv_layer_4 = Conv2D(1, (3, 3), activation='sigmoid', padding='same')

decoded = decode_conv_layer_1(encoded)   # DecoderëŠ” Encoderì˜ ì¶œë ¥ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.
decoded = decode_upsample_layer_1(decoded)
decoded = decode_conv_layer_2(decoded)
decoded = decode_upsample_layer_2(decoded)
decoded = decode_conv_layer_3(decoded)
decoded = decode_upsample_layer_3(decoded)
decoded = decode_conv_layer_4(decoded)

# AutoEncoder ëª¨ë¸ ì •ì˜
autoencoder=Model(input_img, decoded)
autoencoder.summary()

autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')

autoencoder.fit(x_train, x_train,
                epochs=50,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test, x_test))

x_test_10 = x_test[:10]       # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ 10ê°œë§Œ ê³¨ë¼ì„œ
x_test_hat = autoencoder.predict(x_test_10)    # AutoEncoder ëª¨ë¸ì˜ ì´ë¯¸ì§€ ë³µì›ìƒì„±
x_test_imgs = x_test_10.reshape(-1, 28, 28)
x_test_hat_imgs = x_test_hat.reshape(-1, 28, 28)

plt.figure(figsize=(12,5))  # ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ ì§€ì •
for i in range(10):  
    # ì›ë³¸ì´ë¯¸ì§€ ì¶œë ¥
    plt.subplot(2, 10, i+1)
    plt.imshow(x_test_imgs[i])
    # ìƒì„±ëœ ì´ë¯¸ì§€ ì¶œë ¥
    plt.subplot(2, 10, i+11)
    plt.imshow(x_test_hat_imgs[i])
```

ì—¬ê¸°ì„œ ì£¼ì˜í•  ì ì€ Conv2D ë ˆì´ì–´ëŠ” shapeë¥¼ ë³€í™”ì‹œí‚¤ì§€ ì•Šê³  ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤.

Output shapeë¥¼ ë³€í™”ì‹œí‚¤ëŠ” ê²ƒì€ ì˜¤ë¡¯ì´ MaxPooling2D ë ˆì´ì–´ì˜ ì—­í• .

### Decoder Layers for Reconstruction

---

AutoEncoder êµ¬ì¡°ê°€ ì–¼ë§ˆë‚˜ ì •ë³´ì†ì‹¤ ì—†ì´ ì›ë³¸ ë°ì´í„°ë¥¼ ì˜ ì••ì¶•í•˜ê³  ìˆëŠëƒì— ë”°ë¼ Decoderê°€ ë½‘ì•„ë‚¼ ìˆ˜ ìˆëŠ” ìµœì¢…ì ì¸ ì´ë¯¸ì§€ì˜ í€„ë¦¬í‹°ê°€ ê²°ì •ë©ë‹ˆë‹¤.

Image Reconstructionì„ ìœ„í•´ì„œ ì¼ë°˜ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©í•˜ëŠ” Transposed Convolutionì„ ë°©ê¸ˆ í™œìš©í•œ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ì˜¤íˆë ¤ ë°©ê¸ˆì€ Convolution ë ˆì´ì–´ë¥¼ í™œìš©í•´ì„œ Transposed Convolutionë¥¼ í‰ë‚´ë‚¸ ê²ƒì— ë¶ˆê³¼í•©ë‹ˆë‹¤.

### Upsampling ë ˆì´ì–´

---

- Nearest Neighbor : ë³µì›í•´ì•¼ í•  ê°’ì„ ê°€ê¹Œìš´ ê°’ìœ¼ë¡œ ë³µì œí•œë‹¤.
- Bed of Nails : ë³µì›í•´ì•¼ í•  ê°’ì„ 0ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.
- Max Unpooling : Max Pooling ë•Œ ë²„ë¦° ê°’ì„ ì‹¤ì€ ë”°ë¡œ ê¸°ì–µí•´ ë‘ì—ˆë‹¤ê°€ ê·¸ ê°’ìœ¼ë¡œ ë³µì›í•œë‹¤.

```python
from tensorflow.python.keras.layers import Conv2DTranspose
# Conv2DTransposeë¥¼ í™œìš©í•œ  AutoEncoder ëª¨ë¸
# AutoEncoder ëª¨ë¸ êµ¬ì„± - Input ë¶€ë¶„
input_shape = x_train.shape[1:]
input_img = Input(shape=input_shape)

# AutoEncoder ëª¨ë¸ êµ¬ì„± - Encoder ë¶€ë¶„
encode_conv_layer_1 = Conv2D(16, (3, 3), activation='relu')
encode_pool_layer_1 = MaxPooling2D((2, 2))
encode_conv_layer_2 = Conv2D(8, (3, 3), activation='relu')
encode_pool_layer_2 = MaxPooling2D((2, 2))
encode_conv_layer_3 = Conv2D(4, (3, 3), activation='relu')

encoded = encode_conv_layer_1(input_img)
encoded = encode_pool_layer_1(encoded)
encoded = encode_conv_layer_2(encoded)
encoded = encode_pool_layer_2(encoded)
encoded = encode_conv_layer_3(encoded)

# AutoEncoder ëª¨ë¸ êµ¬ì„± - Decoder ë¶€ë¶„  - 
decode_conv_layer_1 = Conv2DTranspose(4, (3, 3), activation='relu', padding='same')
decode_upsample_layer_1 = UpSampling2D((2, 2))
decode_conv_layer_2 = Conv2DTranspose(8, (3, 3), activation='relu', padding='same')
decode_upsample_layer_2 = UpSampling2D((2, 2))
decode_conv_layer_3 = Conv2DTranspose(16, (3, 3), activation='relu')
decode_upsample_layer_3 = UpSampling2D((2, 2))
decode_conv_layer_4 = Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')

decoded = decode_conv_layer_1(encoded)   # DecoderëŠ” Encoderì˜ ì¶œë ¥ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.
decoded = decode_upsample_layer_1(decoded)
decoded = decode_conv_layer_2(decoded)
decoded = decode_upsample_layer_2(decoded)
decoded = decode_conv_layer_3(decoded)
decoded = decode_upsample_layer_3(decoded)
decoded = decode_conv_layer_4(decoded)

# AutoEncoder ëª¨ë¸ ì •ì˜
autoencoder=Model(input_img, decoded)
autoencoder.summary()

autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')

autoencoder.fit(x_train, x_train,
                epochs=50,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test, x_test))

x_test_10 = x_test[:10]       # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ 10ê°œë§Œ ê³¨ë¼ì„œ
x_test_hat = autoencoder.predict(x_test_10)    # AutoEncoder ëª¨ë¸ì˜ ì´ë¯¸ì§€ ë³µì›ìƒì„±
x_test_imgs = x_test_10.reshape(-1, 28, 28)
x_test_hat_imgs = x_test_hat.reshape(-1, 28, 28)

plt.figure(figsize=(12,5))  # ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ ì§€ì •
for i in range(10):  
    # ì›ë³¸ì´ë¯¸ì§€ ì¶œë ¥
    plt.subplot(2, 10, i+1)
    plt.imshow(x_test_imgs[i])
    # ìƒì„±ëœ ì´ë¯¸ì§€ ì¶œë ¥
    plt.subplot(2, 10, i+11)
    plt.imshow(x_test_hat_imgs[i])
```

### **Up-sampling with Transposed Convolution**

---

[https://zzsza.github.io/data/2018/06/25/upsampling-with-transposed-convolution/](https://zzsza.github.io/data/2018/06/25/upsampling-with-transposed-convolution/)

3x3 ì»¤ë„ì€ input matrixì˜ 9ê°œì˜ ê°’ì„ output matrix 1ê°œì˜ ê°’ì— ì—°ê²°í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤. convolution ì—°ì‚°ì€ many-to-one ê´€ê³„ë¥¼ í˜•ì„±í•©ë‹ˆë‹¤.

matrixì˜ ê°’ 1ê°œë¥¼ ë‹¤ë¥¸ matrixì˜ ê°’ 9ê°œì™€ ì—°ê²°í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ì´ëŠ” **one-to-many** ê´€ê³„ì…ë‹ˆë‹¤. convolution ì—°ì‚°ì„ ë°˜ëŒ€ë¡œ í•˜ëŠ” ê²ƒê³¼ ê°™ìœ¼ë©°, transposed convolutionì˜ í•µì‹¬ ê°œë…

![images/Untitled%206.png](images/Untitled%206.png)

â‡’ ì´ëŸ° ì—°ì‚´ì„ ì–´ë–»ê²Œ í•˜ëŠ”ê°€? ***transposed convolution matrix***

convolution matrixëŠ” kernel weightsì˜ ì¬ë°°ì¹˜ì¼ ë¿ì´ê³  convolution ì—°ì‚°ì€ convolution matrixë¥¼ ì‚¬ìš©í•´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Transposed convolution ì—°ì‚°ì€ ì¼ë°˜ì ì¸ convolution ì—°ì‚°ê³¼ ë™ì¼í•œ ì—°ê²°ì„±ì„ í˜•ì„±í•˜ì§€ë§Œ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì—°ê²°

í ... ì™„ì „íˆ ì´í•´ëŠ” x

---

# [E-11] ì‘ì‚¬ê°€ ì¸ê³µì§€ëŠ¥ ë§Œë“¤ê¸°

---

## **ëª©ì°¨**

---

- ì‹œí€€ìŠ¤? ìŠ¤í€€ìŠ¤!
- I ë‹¤ìŒ amì„ ì“°ë©´ ë°˜ ì´ìƒì€ ë§ë”ë¼
- ì‹¤ìŠµ1) ë°ì´í„° ë‹¤ë“¬ê¸°2) ì¸ê³µì§€ëŠ¥ í•™ìŠµì‹œí‚¤ê¸°3) ì˜ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€ í‰ê°€í•˜ê¸°
- í”„ë¡œì íŠ¸ : ë©‹ì§„ ì‘ì‚¬ê°€ ë§Œë“¤ê¸°

## ì‹œí€€ìŠ¤? ìŠ¤í€€ìŠ¤!

---

ë°ì´í„°ë¥¼ ìˆœì„œëŒ€ë¡œ í•˜ë‚˜ì”© ë‚˜ì—´í•˜ì—¬ ë‚˜íƒ€ë‚¸ ë°ì´í„° êµ¬ì¡°, íŠ¹ì • ìœ„ì¹˜ì˜ ë°ì´í„°ë¥¼ ê°€í‚¤í‚¬ ìˆ˜ ìˆë‹¤.

ì‹ ë¬¸ê¸°ì‚¬, ì‹œ, ì†Œì„¤ ë“± ìš°ë¦¬ ì£¼ë³€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ëŠ” ì „ë¶€ ì‹œí€€ìŠ¤ ë°ì´í„°ì…ë‹ˆë‹¤.
í…ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë”ë¼ë„, ì›”ë³„ ìƒí’ˆ íŒë§¤ëŸ‰ ë³€í™”, ì¼ë³„ ì£¼ì‹ ì‹œí™© ë°ì´í„° ë“±ì˜ ì‹œê³„ì—´ ìˆ˜ì¹˜ ë°ì´í„°ë„ ì‹œí€€ìŠ¤ ë°ì´í„°ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì‹œí€€ìŠ¤ ë°ì´í„°ê°€ ê³§ ê° ìš”ì†Œë“¤ì˜ ì—°ê´€ì„±ì„ ì˜ë¯¸í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆì§€ë§Œ, ìš°ë¦¬(ì¸ê³µì§€ëŠ¥)ê°€ ì˜ˆì¸¡ì„ í•˜ë ¤ë©´ ì–´ëŠ ì •ë„ëŠ” ì—°ê´€ì„±ì´ ìˆì–´ì¤˜ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, [ 18.01.01, 18.01.02, 18.01.03, **?** ] ì˜ **"?"** ë¶€ë¶„ì„ ë§ì¶”ê¸° ìœ„í•´ì„  ì •ë‹µì´ 18.01.04 ì—¬ì•¼ë§Œ í•©ë‹ˆë‹¤. ì •ë‹µì´ "ì˜¤ë¦¬"ë¼ë©´ ë‚œê°í•˜ë‹¤ëŠ” ê±°ì£ !

ë¬¸ë²•ì„ ì¸ê³µì§€ëŠ¥ì´ ê·¸ëŒ€ë¡œ ë°°ì›Œì„œ ë¬¸ì¥ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê¸°ëŠ” ì–´ë µë‹¤. ì¡°ê¸ˆ ë” ë‹¨ìˆœí•œ ì ‘ê·¼ ë°©ë²•ì¸ 'í†µê³„ì— ê¸°ë°˜í•œ ë°©ë²•' ì´ í•„ìš”í•˜ë‹¤.

### I ë‹¤ìŒ amì„ ì“°ë©´ ë°˜ ì´ìƒì€ ë§ë”ë¼.

---

í†µê³„ë¼ëŠ” ë‹¨ì–´ì—ì„œ ê±°ë¶€ê°ì´ ëŠê»´ì§€ëŠ” ë¶„ë“¤ì´ ë¶„ëª… ê³„ì‹¤ ê±°ë¼ ìƒê°í•´ìš”. ì ì–´ë„ ì´ë²ˆ ìŠ¤í…ì—ì„œëŠ” ì“¸ ìˆ˜ ìˆëŠ” ê¿€íŒì„ ì „ìˆ˜í•´ë“œë¦¬ë©´ "í†µê³„" â†’ "ëŒ€ì²´ë¡œ~" ë¼ê³  ë°”ê¿” ì½ìœ¼ì‹œë©´ ì¢‹ìŠµë‹ˆë‹¤.

ì¸ê³µì§€ëŠ¥ì´ ê¸€ì„ ì´í•´í•˜ê²Œ í•˜ëŠ” ë°©ì‹ë„ ìœ„ì™€ ê°™ìŠµë‹ˆë‹¤. ì–´ë–¤ ë¬¸ë²•ì ì¸ ì›ë¦¬ë¥¼ í†µí•´ì„œê°€ ì•„ë‹ˆê³ , **ìˆ˜ë§ì€ ê¸€ì„ ì½ê²Œ í•¨ìœ¼ë¡œì¨ ë‚˜ëŠ” , ë°¥ì„, ê·¸ ë‹¤ìŒì´ ë¨¹ëŠ”ë‹¤ ë¼ëŠ” ì‚¬ì‹¤ì„ ì•Œê²Œ í•˜ëŠ” ê±°ì£ .**

â‡’ ë§ì€ ë°ì´í„°ê°€ ê³§ ì¢‹ì€ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤.

ì´ ë°©ì‹ì„ ê°€ì¥ ì˜ ì²˜ë¦¬í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ì¤‘ í•˜ë‚˜ê°€ **ìˆœí™˜ì‹ ê²½ë§(RNN)** ì…ë‹ˆë‹¤.

<start> ë¼ëŠ” íŠ¹ìˆ˜í•œ í† í°ì„ ë§¨ ì•ì— ì¶”ê°€í•´ì¤€ë‹¤. ì¸ê³µì§€ëŠ¥ì—ê²Œ "ì, ì´ì œ ì–´ë–¤ ë¬¸ì¥ì´ë“  ìƒì„±í•´ë´!" ë¼ëŠ” ì‚¬ì¸ì„ ì£¼ëŠ” ì…ˆì¸ê±°ì£ . <start> ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì€ ìˆœí™˜ì‹ ê²½ë§ì€ ë‹¤ìŒ ë‹¨ì–´ë¡œ ë‚˜ëŠ” ì„ ìƒì„±í•˜ê³ , ***ìƒì„±í•œ ë‹¨ì–´ë¥¼ ë‹¤ì‹œ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©*** í•©ë‹ˆë‹¤. ì´ ìˆœí™˜ì ì¸ íŠ¹ì„±ì„ ì‚´ë ¤ ìˆœí™˜ì‹ ê²½ë§ì´ë¼ê³  ì´ë¦„ì„ ë¶™ì¸ ê²ƒ!

ê·¸ë¦¬ê³  ë¬¸ì¥ì„ ë‹¤ ìƒì„±í•˜ë©´ <end> ë¼ëŠ” íŠ¹ìˆ˜í•œ í† í°ì„ ìƒì„±.

ì¦‰, ìš°ë¦¬ëŠ” **<start> ê°€ ë¬¸ì¥ì˜ ì‹œì‘ì— ë”í•´ì§„ ì…ë ¥ ë°ì´í„°(ë¬¸ì œì§€)**ì™€, **<end> ê°€ ë¬¸ì¥ì˜ ëì— ë”í•´ì§„ ì¶œë ¥ ë°ì´í„°(ë‹µì•ˆì§€)**ê°€ í•„ìš”í•˜ë©°, ì´ëŠ” ë¬¸ì¥ ë°ì´í„°ë§Œ ìˆìœ¼ë©´ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ ë˜í•œ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
sentence = " ë‚˜ëŠ” ë°¥ì„ ë¨¹ì—ˆë‹¤ "

source_sentence = "<start>" + sentence
target_sentence = sentence + "<end>"

print("Source ë¬¸ì¥:", source_sentence)
print("Target ë¬¸ì¥:", target_sentence)
```

### ì–¸ì–´ ëª¨ë¸ (Language Model)

---

ì´ê±¸ ì¢€ë” í™•ë¥ ì ìœ¼ë¡œ í‘œí˜„í•´ ë³´ê² ìŠµë‹ˆë‹¤. 'ë‚˜ëŠ” ë°¥ì„' ë‹¤ìŒì— 'ë¨¹ì—ˆë‹¤' ê°€ ë‚˜ì˜¬ í™•ë¥ ì„$p(ë¨¹ì—ˆë‹¤ | ë‚˜ëŠ”, ë°¥ì„)$ ë¼ê³  í•´ë³´ì.

ê·¸ë ‡ë‹¤ë©´, ì´ í™•ë¥ ì€ 'ë‚˜ëŠ”' ë’¤ì— 'ë°¥ì´' ê°€ ë‚˜ì˜¬ í™•ë¥ ì¸ $p(ë°¥ì„|ë‚˜ëŠ”)$ ë³´ë‹¤ëŠ” ë†’ê²Œ ë‚˜ì˜¬ ê²ƒ.

ì•„ë§ˆ $p(ë¨¹ì—ˆë‹¤ | ë‚˜ëŠ”, ë°¥ì„, ë§›ìˆê²Œ)$ ì˜ í™•ë¥ ê°’ì€ ë” ë†’ì•„ì§ˆê±°ê³ .

***ì–´ë–¤ ë¬¸êµ¬ ë’¤ì— ë‹¤ìŒ ë‹¨ì–´ê°€ ë‚˜ì˜¬ í™•ë¥ ì´ ë†’ë‹¤ëŠ” ê²ƒì€ ê·¸ ë‹¤ìŒ ë‹¨ì–´ê°€ ë‚˜ì˜¤ëŠ” ê²ƒì´ ë³´ë‹¤ ìì—°ìŠ¤ëŸ½ë‹¤ëŠ” ëœ»***

n-1ê°œì˜ ë‹¨ì–´ ì‹œí€€ìŠ¤ $w_1 , \cdots , w_{n-1}$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ***n ë²ˆì§¸ ë‹¨ì–´ $w_n$ ìœ¼ë¡œ ë¬´ì—‡ì´ ì˜¬ì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í™•ë¥  ëª¨ë¸ì„ 'ì–¸ì–´ ëª¨ë¸(Language Model)ì´ë¼ê³  ë¶€ë¥¸ë‹¤.*** 

$P(w_n | w_1, â€¦, w_{n-1};\theta )$

ì–´ë–¤ í…ìŠ¤íŠ¸ë„ ì–¸ì–´ ëª¨ë¸ì˜ í•™ìŠµ ë°ì´í„°ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. n-1ë²ˆì§¸ê¹Œì§€ì˜ ë‹¨ì–´ ì‹œí€€ìŠ¤ê°€ x_trainì´ ë˜ê³  në²ˆì§¸ ë‹¨ì–´ê°€ y_trainì´ ë˜ëŠ” ë°ì´í„°ì…‹ì€ ë¬´ê¶ë¬´ì§„í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë‹ˆê¹Œìš”.

ì´ë ‡ê²Œ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œê°€ ì•„ë‹Œ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ê°€ë™í•˜ë©´ ì–´ë–¤ ì¼ì´ ë²Œì–´ì§ˆê¹Œìš”? ë„¤, ì´ ëª¨ë¸ì€ ì¼ì •í•œ ë‹¨ì–´ ì‹œí€€ìŠ¤ê°€ ì£¼ì–´ì§„ë‹¤ë©´ ë‹¤ìŒ ë‹¨ì–´, ê·¸ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ê³„ì†í•´ì„œ ì˜ˆì¸¡í•´ ë‚¼ ê²ƒì…ë‹ˆë‹¤. ì´ê²Œ ë°”ë¡œ í…ìŠ¤íŠ¸ ìƒì„±ì´ê³  ì‘ë¬¸ ì•„ë‹ˆê² ìŠµë‹ˆê¹Œ? ì˜ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì€ í›Œë¥­í•œ ë¬¸ì¥ ìƒì„±ê¸°ë¡œ ë™ì‘í•˜ê²Œ ë©ë‹ˆë‹¤.

## ì‹¤ìŠµ (1) ë°ì´í„° ë‹¤ë“¬ê¸°

---

```python
for idx, sentence in enumerate(raw_corpus):
    if len(sentence) == 0: continue   # ê¸¸ì´ê°€ 0ì¸ ë¬¸ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤.
    if sentence[-1] == ":": continue  # ë¬¸ì¥ì˜ ëì´ : ì¸ ë¬¸ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤.

    if idx > 9: break   # ì¼ë‹¨ ë¬¸ì¥ 10ê°œë§Œ í™•ì¸í•´ ë³¼ ê²ë‹ˆë‹¤.
        
    print(sentence)
```

### í† í°í™”(Tokenize)

---

í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì—ì„œë„ ë‹¨ì–´ ì‚¬ì „ì„ ë§Œë“ ë‹¤. ë¬¸ì¥ì„ ì¼ì •í•œ ê¸°ì¤€ìœ¼ë¡œ ìª¼ê°œì•¼ í•œë‹¤. ê·¸ ê³¼ì •ì„ í† í°í™”.

ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ ë„ì–´ì“°ê¸° ê¸°ì¤€. í•˜ì§€ë§Œ ë¬¸ì œì  ì¡´ì¬.

1. Hi, my name is John. *("Hi," "my", â€¦, "john." ìœ¼ë¡œ ë¶„ë¦¬ë¨) - ë¬¸ì¥ë¶€í˜¸
2. First, open the first chapter. *(Firstì™€ firstë¥¼ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ì¸ì‹) - ëŒ€ì†Œë¬¸ì
3. He is a ten-year-old boy. *(ten-year-oldë¥¼ í•œ ë‹¨ì–´ë¡œ ì¸ì‹) - íŠ¹ìˆ˜ë¬¸ì

"1." ì„ ë§‰ê¸° ìœ„í•´ ë¬¸ì¥ ë¶€í˜¸ ì–‘ìª½ì— ê³µë°±ì„ ì¶”ê°€ í•  ê±°ê³ ìš”, 

"2." ë¥¼ ë§‰ê¸° ìœ„í•´ ëª¨ë“  ë¬¸ìë“¤ì„ ì†Œë¬¸ìë¡œ ë³€í™˜í•  ê²ë‹ˆë‹¤.

"3." ì„ ë§‰ê¸° ìœ„í•´ íŠ¹ìˆ˜ë¬¸ìë“¤ì€ ëª¨ë‘ ì œê±°í•˜ë„ë¡ í•˜ì£ !

### ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•œ í•„í„°ë§ ( corpus ìƒì„± )

---

```python
def preprocess_sentence(sentence):
    sentence = sentence.lower().strip()       # ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³  ì–‘ìª½ ê³µë°±ì„ ì‚­ì œ
  
    # ì•„ë˜ 3ë‹¨ê³„ë¥¼ ê±°ì³ sentenceëŠ” ìŠ¤í˜ì´ìŠ¤ 1ê°œë¥¼ delimeterë¡œ í•˜ëŠ” ì†Œë¬¸ì ë‹¨ì–´ ì‹œí€€ìŠ¤ë¡œ ë°”ë€ë‹ˆë‹¤.
    sentence = re.sub(r"([?.!,Â¿])", r" \1 ", sentence)        # íŒ¨í„´ì˜ íŠ¹ìˆ˜ë¬¸ìë¥¼ ë§Œë‚˜ë©´ íŠ¹ìˆ˜ë¬¸ì ì–‘ìª½ì— ê³µë°±ì„ ì¶”ê°€
    sentence = re.sub(r'[" "]+', " ", sentence)                  # ê³µë°± íŒ¨í„´ì„ ë§Œë‚˜ë©´ ìŠ¤í˜ì´ìŠ¤ 1ê°œë¡œ ì¹˜í™˜
    sentence = re.sub(r"[^a-zA-Z?.!,Â¿]+", " ", sentence)  # a-zA-Z?.!,Â¿ íŒ¨í„´ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì(ê³µë°±ë¬¸ìê¹Œì§€ë„)ë¥¼ ìŠ¤í˜ì´ìŠ¤ 1ê°œë¡œ ì¹˜í™˜

    sentence = sentence.strip()

    sentence = '<start> ' + sentence + ' <end>'      # ì´ì „ ìŠ¤í…ì—ì„œ ë³¸ ê²ƒì²˜ëŸ¼ ë¬¸ì¥ ì•ë’¤ë¡œ <start>ì™€ <end>ë¥¼ ë‹¨ì–´ì²˜ëŸ¼ ë¶™ì—¬ ì¤ë‹ˆë‹¤
    
    return sentence

print(preprocess_sentence("This @_is ;;;sample        sentence."))   # ì´ ë¬¸ì¥ì´ ì–´ë–»ê²Œ í•„í„°ë§ë˜ëŠ”ì§€ í™•ì¸í•´ ë³´ì„¸ìš”.
```

ìì—°ì–´ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ ëª¨ë¸ì˜ ì…ë ¥ì´ ë˜ëŠ” ë¬¸ì¥ì„ **ì†ŒìŠ¤ ë¬¸ì¥(Source Sentence),** ì •ë‹µ ì—­í• ì„ í•˜ê²Œ ë  ëª¨ë¸ì˜ ì¶œë ¥ ë¬¸ì¥ì„ **íƒ€ê²Ÿ ë¬¸ì¥(Target Sentence)**ë¼ê³  ê´€ë¡€ì ìœ¼ë¡œ ë¶€ë¦…ë‹ˆë‹¤. ê°ê° X_train, y_train ì— í•´ë‹¹í•œë‹¤ê³  í•  ìˆ˜ ìˆê² ì£ ?

â‡’ ì •ì œ í•¨ìˆ˜ë¥¼ í†µí•´ ë§Œë“  ë°ì´í„°ì…‹ì—ì„œ í† í°í™”ë¥¼ ì§„í–‰í•œ í›„ ë ë‹¨ì–´ ë¥¼ ì—†ì• ë©´ ì†ŒìŠ¤ ë¬¸ì¥, ì²« ë‹¨ì–´ ë¥¼ ì—†ì• ë©´ íƒ€ê²Ÿ ë¬¸ì¥ì´ ë˜ê² ì£ ?

```python
corpus = []

for sentence in raw_corpus:
    if len(sentence) == 0: continue
    if sentence[-1] == ":": continue
        
    corpus.append(preprocess_sentence(sentence))
        
corpus[:10]
```

ì˜ì–´ë¥¼ ì²˜ìŒ ë°°ìš¸ ë•Œë¥¼ ìƒê°í•´ë³´ë©´... í•œêµ­ì–´ í•´ì„ì„ ë³´ë©´ì„œ ê³µë¶€í–ˆì–ì•„?

ì¸ê³µì§€ëŠ¥ë„ ë°°ìš°ê³ ì í•˜ëŠ” ì–¸ì–´ ë¥¼ ëª¨êµ­ì–´ë¡œ í‘œí˜„ ì„ í•´ì•¼ ê³µë¶€ë¥¼ í•  ìˆ˜ ìˆì–´ìš”.

â‡’ ì¸ê³µì§€ëŠ¥ì˜ ëª¨êµ­ì–´ëŠ” ìˆ«ì! (í‘œí˜„ì´ ì¢‹ë‹¤!)

â‡’ ê°€ë¥´ì¹  ì–¸ì–´(ë°ì´í„°)ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•´ì„œ ì¸ê³µì§€ëŠ¥ì—ê²Œ ì¤„ ê²ë‹ˆë‹¤. ì´ì— í•„ìš”í•œ ê²ƒì€ ì‚¬ì „!

### ë²¡í„°í™”, í…ì„œ

---

`tf.keras.preprocessing.text.Tokenizer` íŒ¨í‚¤ì§€ëŠ” ì •ì œëœ ë°ì´í„°ë¥¼ í† í°í™”í•˜ê³ , ë‹¨ì–´ ì‚¬ì „(vocabulary ë˜ëŠ” dictionaryë¼ê³  ì¹­í•¨)ì„ ë§Œë“¤ì–´ì£¼ë©°, ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜ê¹Œì§€ í•œ ë°©ì— í•´ì¤ë‹ˆë‹¤.

â‡’ ì´ ê³¼ì •ì„ ë²¡í„°í™”(vectorize) ë¼ê³  í•˜ë©°, ìˆ«ìë¡œ ë³€í™˜ëœ ë°ì´í„°ë¥¼ í…ì„œ(tensor) ë¼ê³  ì¹­í•œë‹¤.

### í…ì„œ?? ê°„ë‹¨í•œ ì„¤ëª…

---

[https://rekt77.tistory.com/102](https://rekt77.tistory.com/102)

ë°ì´í„°ì˜ ë°°ì—´

í…ì„œì˜ RankëŠ” ê°„ë‹¨íˆ ë§í•´ì„œ ëª‡ ì°¨ì› ë°°ì—´ì¸ê°€ë¥¼ ì˜ë¯¸

![images/Untitled%207.png](images/Untitled%207.png)

**1. TensorëŠ” ë°°ì—´ì˜ ì§‘í•©ì´ë‹¤.**

**2. ì°¨ì›ì˜ ìˆ˜ëŠ” Rankì™€ ê°™ì€ë§ì´ë‹¤.**

**3. ë°°ì—´ì˜ ì°¨ì›ì—ë”°ë¼ ë¶ˆë¦¬ëŠ” ì´ë¦„ì´ ë‹¬ë¼ì§„ë‹¤.**

### í…ì„œ ìƒì„±

---

```python
def tokenize(corpus):
    # í…ì„œí”Œë¡œìš°ì—ì„œ ì œê³µí•˜ëŠ” Tokenizer íŒ¨í‚¤ì§€ë¥¼ ìƒì„±
    tokenizer = tf.keras.preprocessing.text.Tokenizer(
        num_words=7000,  # ì „ì²´ ë‹¨ì–´ì˜ ê°œìˆ˜ 
        filters=' ',    # ë³„ë„ë¡œ ì „ì²˜ë¦¬ ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë²ˆì—ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê² ìŠµë‹ˆë‹¤.
        oov_token="<unk>"  # out-of-vocabulary, ì‚¬ì „ì— ì—†ì—ˆë˜ ë‹¨ì–´ëŠ” ì–´ë–¤ í† í°ìœ¼ë¡œ ëŒ€ì²´í• ì§€
    )
    tokenizer.fit_on_texts(corpus)   # ìš°ë¦¬ê°€ êµ¬ì¶•í•œ corpusë¡œë¶€í„° Tokenizerê°€ ì‚¬ì „ì„ ìë™êµ¬ì¶•í•˜ê²Œ ë©ë‹ˆë‹¤.

    # ì´í›„ tokenizerë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì— ì…ë ¥í•  ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ê²Œ ë©ë‹ˆë‹¤.
    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizerëŠ” êµ¬ì¶•í•œ ì‚¬ì „ìœ¼ë¡œë¶€í„° corpusë¥¼ í•´ì„í•´ Tensorë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    # ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶”ê¸° ìœ„í•œ padding  ë©”ì†Œë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    # maxlenì˜ ë””í´íŠ¸ê°’ì€ Noneì…ë‹ˆë‹¤. ì´ ê²½ìš° corpusì˜ ê°€ì¥ ê¸´ ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ë§ì¶°ì§‘ë‹ˆë‹¤.
    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  

    print(tensor,tokenizer)
    return tensor, tokenizer

tensor, tokenizer = tokenize(corpus)
```

ìƒì„±ëœ í…ì„œë¥¼ ì†ŒìŠ¤ì™€ íƒ€ê²Ÿìœ¼ë¡œ ë¶„ë¦¬.

```python
src_input = tensor[:, :-1]  # tensorì—ì„œ ë§ˆì§€ë§‰ í† í°ì„ ì˜ë¼ë‚´ì„œ ì†ŒìŠ¤ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ í† í°ì€ <END>ê°€ ì•„ë‹ˆë¼ <pad>ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
tgt_input = tensor[:, 1:]    # tensorì—ì„œ <START>ë¥¼ ì˜ë¼ë‚´ì„œ íƒ€ê²Ÿ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.

print(src_input[0])
print(tgt_input[0])
```

### ë°ì´í„°ì…‹ ê°ì²´ ìƒì„±

---

ê·¸ë™ì•ˆ model.fit(xtrain, ytrain, â€¦) í˜•íƒœë¡œ Numpy Array ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ì—¬ modelì— ì œê³µí•˜ëŠ” í˜•íƒœì˜ í•™ìŠµì„ ë§ì´ ì§„í–‰í•´ ì™”ìŠµë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ í…ì„œí”Œë¡œìš°ë¥¼ í™œìš©í•  ê²½ìš° í…ì„œë¡œ ìƒì„±ëœ ë°ì´í„°ë¥¼ ì´ìš©í•´ `tf.data.Dataset` ê°ì²´ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ í”íˆ ì‚¬ìš©.

`tf.data.Dataset` ê°ì²´ëŠ” í…ì„œí”Œë¡œìš°ì—ì„œ ì‚¬ìš©í•  ê²½ìš° ë°ì´í„° ì…ë ¥ íŒŒì´í”„ë¼ì¸ì„ í†µí•œ ì†ë„ ê°œì„  ë° ê°ì¢… í¸ì˜ê¸°ëŠ¥ì„ ì œê³µí•˜ë¯€ë¡œ ê¼­ ì‚¬ìš©ë²•ì„ ì•Œì•„ ë‘ì‹œê¸°ë¥¼ ê¶Œí•©ë‹ˆë‹¤.

```python
BUFFER_SIZE = len(src_input)
BATCH_SIZE = 256
steps_per_epoch = len(src_input) // BATCH_SIZE

VOCAB_SIZE = tokenizer.num_words + 1    # tokenizerê°€ êµ¬ì¶•í•œ ë‹¨ì–´ì‚¬ì „ ë‚´ 7000ê°œì™€, ì—¬ê¸° í¬í•¨ë˜ì§€ ì•Šì€ 0:<pad>ë¥¼ í¬í•¨í•˜ì—¬ 7001ê°œ

dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)
dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)
dataset
```

ì—¬ê¸°ê¹Œì§€ ê³¼ì • (ë°ì´í„° ì „ì²˜ë¦¬)

---

- ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•œ corpus ìƒì„±
- **`tf.keras.preprocessing.text.Tokenizer`**ë¥¼ ì´ìš©í•´ corpusë¥¼ í…ì„œë¡œ ë³€í™˜
- **`tf.data.Dataset.from_tensor_slices()`**ë¥¼ ì´ìš©í•´ corpus í…ì„œë¥¼Â **`tf.data.Dataset`** ë¡œ ë³€í™˜

## ì‹¤ìŠµ(2) ì¸ê³µì§€ëŠ¥ í•™ìŠµì‹œí‚¤ê¸°

---

![images/Untitled%208.png](images/Untitled%208.png)

ì—¬ê¸°ì„œ ë§Œë“¤ ëª¨ë¸ì˜ êµ¬ì¡°ë„

### ìš°ì„ , Embedding ë ˆì´ì–´

---

```python
class TextGenerator(tf.keras.Model):
    def __init__(self, vocab_size, embedding_size, hidden_size):
        super(TextGenerator, self).__init__()
        
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)
        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)
        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)
        self.linear = tf.keras.layers.Dense(vocab_size)
        
    def call(self, x):
        out = self.embedding(x)
        out = self.rnn_1(out)
        out = self.rnn_2(out)
        out = self.linear(out)
        
        return out
    
embedding_size = 256
hidden_size = 1024
model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)
```

ì…ë ¥ í…ì„œì— ë“¤ì–´ìˆëŠ” ë‹¨ì–´ ì‚¬ì „ì˜ ì¸ë±ìŠ¤ ê°’ì„ í•´ë‹¹ ì¸ë±ìŠ¤ ë²ˆì§¸ì˜ ì›Œë“œ ë²¡í„°ë¡œ ë³€í™˜.

ìœ„ ì½”ë“œì—ì„œÂ **`embedding_size`**Â ëŠ” ì›Œë“œ ë²¡í„°ì˜ ì°¨ì›ìˆ˜, ì¦‰ **ë‹¨ì–´ê°€ ì¶”ìƒì ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” í¬ê¸°**ì…ë‹ˆë‹¤. ë§Œì•½ ê·¸ í¬ê¸°ê°€ 2ë¼ë©´ ì˜ˆë¥¼ ë“¤ì–´

- ì°¨ê°‘ë‹¤: [0.0, 1.0]
- ëœ¨ê²ë‹¤: [1.0, 0.0]
- ë¯¸ì§€ê·¼í•˜ë‹¤: [0.5, 0.5]

ê°’ì´ ì»¤ì§ˆìˆ˜ë¡ ë‹¨ì–´ì˜ ì¶”ìƒì ì¸ íŠ¹ì§•ë“¤ì„ ë” ì¡ì•„ë‚¼ ìˆ˜ ìˆì§€ë§Œ, ê·¸ë§Œí¼ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì£¼ì–´ì§€ì§€ ì•Šìœ¼ë©´ ì˜¤íˆë ¤ í˜¼ë€ë§Œì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

LSTM ë ˆì´ì–´ì˜ hidden state ì˜ ì°¨ì›ìˆ˜ì¸ hidden_size ë„ ê°™ì€ ë§¥ë½ì…ë‹ˆë‹¤. hidden_size ëŠ” **ëª¨ë¸ì— ì–¼ë§ˆë‚˜ ë§ì€ ì¼ê¾¼**ì„ ë‘˜ ê²ƒì¸ê°€? ë¡œ ì´í•´í•´ë„ í¬ê²Œ ì—‡ë‚˜ê°€ì§€ ì•ŠìŠµë‹ˆë‹¤.

ê·¸ ì¼ê¾¼ë“¤ì€ ëª¨ë‘ ê°™ì€ ë°ì´í„°ë¥¼ ë³´ê³  **ê°ìì˜ ìƒê°**ì„ ê°€ì§€ëŠ”ë°, ì—­ì‹œ ì¶©**ë¶„í•œ ë°ì´í„°ê°€ ì£¼ì–´ì§€ë©´ ì˜¬ë°”ë¥¸ ê²°ì •ì„ ë‚´ë¦¬ê² ì§€ë§Œ ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë°°ê°€ ì‚°ìœ¼ë¡œ ê°ˆ ë¿** ì…ë‹ˆë‹¤.

ìš°ë¦¬ì˜ modelì€ ì•„ì§ ì œëŒ€ë¡œ buildë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. model.compile()ì„ í˜¸ì¶œí•œ ì ë„ ì—†ê³ , ì•„ì§ modelì˜ ì…ë ¥ í…ì„œê°€ ë¬´ì—‡ì¸ì§€ ì œëŒ€ë¡œ ì§€ì •í•´ ì£¼ì§€ë„ ì•Šì•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

ê·¸ëŸ° ê²½ìš° ì•„ë˜ì™€ ê°™ì´ modelì— ë°ì´í„°ë¥¼ ì•„ì£¼ ì¡°ê¸ˆ íƒœì›Œ ë³´ëŠ” ê²ƒë„ ë°©ë²•ì…ë‹ˆë‹¤. modelì˜ input shapeê°€ ê²°ì •ë˜ë©´ì„œ model.build()ê°€ ìë™ìœ¼ë¡œ í˜¸ì¶œë©ë‹ˆë‹¤.

```python
for src_sample, tgt_sample in dataset.take(1): break
model(src_sample)

# <tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy= array(~~) >
```

7001ì€ Dense ë ˆì´ì–´ì˜ ì¶œë ¥ ì°¨ì›ìˆ˜ì…ë‹ˆë‹¤. 7001ê°œì˜ ë‹¨ì–´ ì¤‘ ì–´ëŠ ë‹¨ì–´ì˜ í™•ë¥ ì´ ê°€ì¥ ë†’ì„ì§€ë¥¼ ëª¨ë¸ë§í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

256ì€ ì´ì „ ìŠ¤í…ì—ì„œ ì§€ì •í•œ ë°°ì¹˜ ì‚¬ì´ì¦ˆì…ë‹ˆë‹¤.

```
dataset.take(1)
```

ë¥¼ í†µí•´ì„œ 1ê°œì˜ ë°°ì¹˜, ì¦‰ 256ê°œì˜ ë¬¸ì¥ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¨ ê²ƒì…ë‹ˆë‹¤.

ê·¸ë ‡ë‹¤ë©´ 20ì€ ë¬´ì—‡ì„ ì˜ë¯¸í• ê¹Œìš”?

ë¹„ë°€ì€ ë°”ë¡œ `tf.keras.layers.LSTM(hidden_size, return_sequences=True)`ë¡œ í˜¸ì¶œí•œ LSTM ë ˆì´ì–´ì—ì„œ `return_sequences=True`ì´ë¼ê³  ì§€ì •í•œ ë¶€ë¶„ì— ìˆìŠµë‹ˆë‹¤. ì¦‰, LSTMì€ ìì‹ ì—ê²Œ ì…ë ¥ëœ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë§Œí¼ ë™ì¼í•œ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë¥¼ ì¶œë ¥í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.

`return_sequences=False`ì˜€ë‹¤ë©´ LSTM ë ˆì´ì–´ëŠ” 1ê°œì˜ ë²¡í„°ë§Œ ì¶œë ¥í–ˆì„ ê²ƒ

***ê·¸ëŸ°ë° ë¬¸ì œëŠ”*** 

ìš°ë¦¬ì˜ ëª¨ë¸ì€ ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì–¼ë§ˆì¸ì§€ ëª¨ë¥¸ë‹¤ëŠ” ì ì…ë‹ˆë‹¤.

ëª¨ë¸ì„ ë§Œë“¤ë©´ì„œ ì•Œë ¤ì¤€ ì ë„ ì—†ìŠµë‹ˆë‹¤. ê·¸ëŸ¼ 20ì€ ì–¸ì œ ì•Œê²Œëœ ê²ƒì¼ê¹Œìš”? ë„¤, ê·¸ë ‡ìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ ì…ë ¥ë°›ìœ¼ë©´ì„œ ë¹„ë¡œì†Œ ì•Œê²Œ ëœ ê²ƒì…ë‹ˆë‹¤. ìš°ë¦¬ ë°ì´í„°ì…‹ì˜ max_lenì´ 20ìœ¼ë¡œ ë§ì¶°ì ¸ ìˆì—ˆë˜ ê²ƒì…ë‹ˆë‹¤.

```python
model.summary()
```

Output Shapeë¥¼ ì •í™•í•˜ê²Œ ì•Œë ¤ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°”ë¡œ ìœ„ì—ì„œ ì„¤ëª…í•œ ì´ìœ  ë•Œë¬¸ì…ë‹ˆë‹¤. ìš°ë¦¬ì˜ ëª¨ë¸ì€ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ ëª¨ë¥´ê¸° ë•Œë¬¸ì— Output Shapeë¥¼ íŠ¹ì •í•  ìˆ˜ ì—†ëŠ” ê²ƒì…ë‹ˆë‹¤.

loss ì¸¡ì •

```python
optimizer = tf.keras.optimizers.Adam()
loss = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True,
    reduction='none'
)

model.compile(loss=loss, optimizer=optimizer)
model.fit(dataset, epochs=30)
```

## ëª¨ë¸ í‰ê°€í•˜ê¸°

---

ì‘ë¬¸ ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•ì€ ì‘ë¬¸ì„ ì‹œì¼œë³´ê³  ì§ì ‘ í‰ê°€í•˜ëŠ” ê²ë‹ˆë‹¤.

```python
def generate_text(model, tokenizer, init_sentence="<start>", max_len=20):
    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œ ì…ë ¥ë°›ì€ init_sentenceë„ ì¼ë‹¨ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    test_input = tokenizer.texts_to_sequences([init_sentence])
    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)
    end_token = tokenizer.word_index["<end>"]

    # í…ìŠ¤íŠ¸ë¥¼ ì‹¤ì œë¡œ ìƒì„±í• ë•ŒëŠ” ë£¨í”„ë¥¼ ëŒë©´ì„œ ë‹¨ì–´ í•˜ë‚˜ì”© ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. 
    while True:
        predict = model(test_tensor)  # ì…ë ¥ë°›ì€ ë¬¸ì¥ì˜ í…ì„œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤. 
        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # ìš°ë¦¬ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë§ˆì§€ë§‰ ë‹¨ì–´ê°€ ë°”ë¡œ ìƒˆë¡­ê²Œ ìƒì„±í•œ ë‹¨ì–´ê°€ ë©ë‹ˆë‹¤. 

        # ìš°ë¦¬ ëª¨ë¸ì´ ìƒˆë¡­ê²Œ ì˜ˆì¸¡í•œ ë‹¨ì–´ë¥¼ ì…ë ¥ ë¬¸ì¥ì˜ ë’¤ì— ë¶™ì—¬ ì¤ë‹ˆë‹¤. 
        test_tensor = tf.concat([test_tensor, 
																 tf.expand_dims(predict_word, axis=0)], axis=-1)

        # ìš°ë¦¬ ëª¨ë¸ì´ <END>ë¥¼ ì˜ˆì¸¡í–ˆê±°ë‚˜, max_lenì— ë„ë‹¬í•˜ì§€ ì•Šì•˜ë‹¤ë©´  while ë£¨í”„ë¥¼ ë˜ ëŒë©´ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•©ë‹ˆë‹¤.
        if predict_word.numpy()[0] == end_token: break
        if test_tensor.shape[1] >= max_len: break

    generated = ""
    # ìƒì„±ëœ tensor ì•ˆì— ìˆëŠ” word indexë¥¼ tokenizer.index_word ì‚¬ì „ì„ í†µí•´ ì‹¤ì œ ë‹¨ì–´ë¡œ í•˜ë‚˜ì”© ë³€í™˜í•©ë‹ˆë‹¤. 
    for word_index in test_tensor[0].numpy():
        generated += tokenizer.index_word[word_index] + " "

    return generated   # ì´ê²ƒì´ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì´ ìƒì„±í•œ ìì—°ì–´ ë¬¸ì¥ì…ë‹ˆë‹¤.
```

â‡’ í…ìŠ¤íŠ¸ë¥¼ ì‹¤ì œë¡œ ìƒì„±í•´ì•¼ í•˜ëŠ” ì‹œì ì—ì„œ, ìš°ë¦¬ì—ê²ŒëŠ” 2ê°€ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. í•˜ë‚˜ëŠ” íƒ€ê²Ÿ ë¬¸ì¥ì…ë‹ˆë‹¤. ë˜í•˜ë‚˜ëŠ” ë¬´ì—‡ì´ëƒ í•˜ë©´, ì†ŒìŠ¤ ë¬¸ì¥ì…ë‹ˆë‹¤. ìƒê°í•´ ë³´ë©´ ìš°ë¦¬ëŠ” í…ìŠ¤íŠ¸ ìƒì„± íƒœìŠ¤í¬ë¥¼ ìœ„í•´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ë”°ë¡œ ìƒì„±í•œ ì ì´ ì—†ìŠµë‹ˆë‹¤.

generate_text() í•¨ìˆ˜ì—ì„œ init_sentenceë¥¼ ì¸ìë¡œ ë°›ê³ ëŠ” ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ë°›ì€ ì¸ìë¥¼ ì¼ë‹¨ í…ì„œë¡œ ë§Œë“¤ê³  ìˆìŠµë‹ˆë‹¤. ë””í´íŠ¸ë¡œëŠ” ë‹¨ì–´ í•˜ë‚˜ë§Œ ë°›ëŠ”êµ°ìš”.

- whileì˜ ì²«ë²ˆì§¸ ë£¨í”„ì—ì„œ test_tensorì—Â **`<START>`**Â í•˜ë‚˜ë§Œ ë“¤ì–´ê°”ë‹¤ê³  í•©ì‹œë‹¤. ìš°ë¦¬ì˜ ëª¨ë¸ì´ ì¶œë ¥ìœ¼ë¡œ 7001ê°œì˜ ë‹¨ì–´ ì¤‘Â **`A`**ë¥¼ ê³¨ëë‹¤ê³  í•©ì‹œë‹¤.
- whileì˜ ë‘ë²ˆì§¸ ë£¨í”„ì—ì„œ test_tensorì—ëŠ”Â **`<START> A`**ê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤. ê·¸ë˜ì„œ ìš°ë¦¬ì˜ ëª¨ë¸ì´ ê·¸ë‹¤ìŒÂ **`B`**ë¥¼ ê³¨ëë‹¤ê³  í•©ì‹œë‹¤.
- whileì˜ ì„¸ë²ˆì§¸ ë£¨í”„ì—ì„œ test_tensorì—ëŠ”Â **`<START> A B`**ê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤. ê·¸ë˜ì„œâ€¦.. (ì´í•˜ í›„ëµ)

```python
generate_text(model, tokenizer, init_sentence="<start> he")
```

## í”„ë¡œì íŠ¸: ë©‹ì§„ ì‘ì‚¬ê°€ ë§Œë“¤ê¸°

---

glob ëª¨ë“ˆì„ ì‚¬ìš©í•˜ë©´ íŒŒì¼ì„ ì½ì–´ì˜¤ëŠ” ì‘ì—…ì„ í•˜ê¸°ê°€ ì•„ì£¼ ìš©ì´í•´ìš”. glob ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë“  txt íŒŒì¼ì„ ì½ì–´ì˜¨ í›„, raw_corpus ë¦¬ìŠ¤íŠ¸ì— ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì €ì¥í•˜ë„ë¡ í• ê²Œìš”!

```python
import glob
import os

txt_file_path = os.getenv('HOME')+'/aiffel//lyricist/data/lyrics/*'

txt_list = glob.glob(txt_file_path)

raw_corpus = []

# ì—¬ëŸ¬ê°œì˜ txt íŒŒì¼ì„ ëª¨ë‘ ì½ì–´ì„œ raw_corpus ì— ë‹´ìŠµë‹ˆë‹¤.
for txt_file in txt_list:
    with open(txt_file, "r") as f:
        raw = f.read().splitlines()
        raw_corpus.extend(raw)

print("ë°ì´í„° í¬ê¸°:", len(raw_corpus))
print("Examples:\n", raw_corpus[:3])
```

ì´í›„ ì´ë²ˆ ê³¼ì •ì—ì„œ í–ˆë˜ ë‚´ìš© ì§„í–‰

```python
loss = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')
```

â‡’ í•´ë‹¹ lossë¥¼ ì‚¬ìš©í•˜ì—¬ val_loss ê°’ì„ 2.2ìˆ˜ì¤€ìœ¼ë¡œ ì¤„ì—¬ë³´ì

```python
generate_text(lyricist, tokenizer, init_sentence="<start> i love", max_len=20)
```

ê²°ê³¼ë¬¼ : [https://github.com/bluecandle/2020_AIFFEL/blob/master/daily_notes/exploration_codes/e11_code/E11.ipynb](https://github.com/bluecandle/2020_AIFFEL/blob/master/daily_notes/exploration_codes/e11_code/E11.ipynb)